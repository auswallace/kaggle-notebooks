{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":76728,"databundleVersionId":9057646,"sourceType":"competition"},{"sourceId":6478229,"sourceType":"datasetVersion","datasetId":3742543}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **PS4E9: Auto EDA & Modeling**\nThis notebook contains Auto EDA and Auto ML approaches for the regression of used car prices as part of the playground series, season 4, episode 9. In detail, it shows:\n\n* Loading of all relevant datasets and first quick analysis of them\n* Auto EDA through ydata_profiling\n* Feature engineering & preprocessing\n* Analysis of most important features\n* AutoML model training with AutoGluon\n* Submitting predictions using the best model based on the validation\n\n**References:**\n* The competition page: https://www.kaggle.com/competitions/playground-series-s4e9/\n* The original dataset: https://www.kaggle.com/datasets/taeefnajib/used-car-price-prediction-dataset/data","metadata":{}},{"cell_type":"markdown","source":"## **Importing/Installing Libraries**","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"!pip install -q autogluon.tabular","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-09-04T11:05:42.248024Z","iopub.execute_input":"2024-09-04T11:05:42.2484Z","iopub.status.idle":"2024-09-04T11:06:26.332801Z","shell.execute_reply.started":"2024-09-04T11:05:42.24836Z","shell.execute_reply":"2024-09-04T11:06:26.331286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom ydata_profiling import ProfileReport\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import TargetEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.tree import DecisionTreeRegressor, plot_tree\nfrom autogluon.tabular import TabularPredictor\nimport re","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-09-04T11:06:26.335732Z","iopub.execute_input":"2024-09-04T11:06:26.336215Z","iopub.status.idle":"2024-09-04T11:06:32.6478Z","shell.execute_reply.started":"2024-09-04T11:06:26.336154Z","shell.execute_reply":"2024-09-04T11:06:32.646573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Understanding the data**","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/playground-series-s4e9/train.csv', index_col='id')\ndf_test = pd.read_csv('/kaggle/input/playground-series-s4e9/test.csv', index_col='id')\ndf_original = pd.read_csv('/kaggle/input/used-car-price-prediction-dataset/used_cars.csv')\ndf_sub = pd.read_csv('/kaggle/input/playground-series-s4e9/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2024-09-04T11:08:54.318051Z","iopub.execute_input":"2024-09-04T11:08:54.319735Z","iopub.status.idle":"2024-09-04T11:08:56.045484Z","shell.execute_reply.started":"2024-09-04T11:08:54.319677Z","shell.execute_reply":"2024-09-04T11:08:56.044174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We first refine two columns of the original dataset so that we can use the data in our training:","metadata":{}},{"cell_type":"code","source":"# thanks to https://www.kaggle.com/competitions/playground-series-s4e9/discussion/531333#2975520 for this code snippet\ndf_original[['milage', 'price']] = df_original[['milage', 'price']].map(\n    lambda x: int(''.join(re.findall(r'\\d+', x))))","metadata":{"execution":{"iopub.status.busy":"2024-09-03T16:00:19.638308Z","iopub.execute_input":"2024-09-03T16:00:19.638727Z","iopub.status.idle":"2024-09-03T16:00:19.665528Z","shell.execute_reply.started":"2024-09-03T16:00:19.638688Z","shell.execute_reply":"2024-09-03T16:00:19.664176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we can merge the training data of the competition and the orginal dataset to one big training set:","metadata":{}},{"cell_type":"code","source":"df_train = pd.concat([df_train, df_original]).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T16:00:19.666752Z","iopub.execute_input":"2024-09-03T16:00:19.66712Z","iopub.status.idle":"2024-09-03T16:00:19.7756Z","shell.execute_reply.started":"2024-09-03T16:00:19.667084Z","shell.execute_reply":"2024-09-03T16:00:19.774309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's get some information of this training set:","metadata":{}},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-03T16:00:19.776974Z","iopub.execute_input":"2024-09-03T16:00:19.77736Z","iopub.status.idle":"2024-09-03T16:00:19.797498Z","shell.execute_reply.started":"2024-09-03T16:00:19.777322Z","shell.execute_reply":"2024-09-03T16:00:19.796288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.info()","metadata":{"execution":{"iopub.status.busy":"2024-09-03T16:00:19.798919Z","iopub.execute_input":"2024-09-03T16:00:19.799301Z","iopub.status.idle":"2024-09-03T16:00:19.893786Z","shell.execute_reply.started":"2024-09-03T16:00:19.799264Z","shell.execute_reply":"2024-09-03T16:00:19.892529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the dataset contains 1 numerical and 10 categorical features.\n\nLet's also calculate the percentage of NaNs for each column of the train and test set:","metadata":{}},{"cell_type":"code","source":"df_train.isnull().mean() * 100","metadata":{"execution":{"iopub.status.busy":"2024-09-03T16:00:19.895548Z","iopub.execute_input":"2024-09-03T16:00:19.895954Z","iopub.status.idle":"2024-09-03T16:00:19.985502Z","shell.execute_reply.started":"2024-09-03T16:00:19.895907Z","shell.execute_reply":"2024-09-03T16:00:19.984199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.isnull().mean() * 100","metadata":{"execution":{"iopub.status.busy":"2024-09-03T16:00:19.98688Z","iopub.execute_input":"2024-09-03T16:00:19.987998Z","iopub.status.idle":"2024-09-03T16:00:20.051466Z","shell.execute_reply.started":"2024-09-03T16:00:19.987788Z","shell.execute_reply":"2024-09-03T16:00:20.050282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that we have some NANs we have to take care of later. In addition, let's check for duplicated rows in train and test set:","metadata":{}},{"cell_type":"code","source":"df_train.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2024-09-03T16:00:20.05526Z","iopub.execute_input":"2024-09-03T16:00:20.055659Z","iopub.status.idle":"2024-09-03T16:00:20.23811Z","shell.execute_reply.started":"2024-09-03T16:00:20.055622Z","shell.execute_reply":"2024-09-03T16:00:20.236745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2024-09-03T16:00:20.239619Z","iopub.execute_input":"2024-09-03T16:00:20.240098Z","iopub.status.idle":"2024-09-03T16:00:20.357426Z","shell.execute_reply.started":"2024-09-03T16:00:20.240049Z","shell.execute_reply":"2024-09-03T16:00:20.356374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are not duplicates in the datasets which is good :). Let's also check how many identical rows appear in both train and test set:","metadata":{}},{"cell_type":"code","source":"len(pd.merge(df_train.drop(columns=['price']), df_test))","metadata":{"execution":{"iopub.status.busy":"2024-09-03T16:00:20.358645Z","iopub.execute_input":"2024-09-03T16:00:20.358986Z","iopub.status.idle":"2024-09-03T16:00:20.740978Z","shell.execute_reply.started":"2024-09-03T16:00:20.358952Z","shell.execute_reply":"2024-09-03T16:00:20.73979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ok, that's interesting and probably results from having added the original dataset to the training data. Let's summarize our first insights:","metadata":{}},{"cell_type":"markdown","source":"####  **First insights into the dataset:**\n\n* We have missing values which we have to impute\n* We have categorical data which we have to encode\n* Finally, we also have features (e.g. 'engine') which contain a lot of potential for feature engineering","metadata":{}},{"cell_type":"markdown","source":"## **EDA (Exploratory Data Analysis)**","metadata":{}},{"cell_type":"markdown","source":"Let's now do an EDA to deepen our understanding of the dataset. We will use the Auto EDA tool ydata_profiling for this purpose:","metadata":{}},{"cell_type":"code","source":"profile = ProfileReport(df_train, title=\"Profiling Report\")","metadata":{"execution":{"iopub.status.busy":"2024-09-03T16:00:20.742463Z","iopub.execute_input":"2024-09-03T16:00:20.74291Z","iopub.status.idle":"2024-09-03T16:00:20.948601Z","shell.execute_reply.started":"2024-09-03T16:00:20.742862Z","shell.execute_reply":"2024-09-03T16:00:20.947357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"profile.to_notebook_iframe()","metadata":{"execution":{"iopub.status.busy":"2024-09-03T16:00:20.949882Z","iopub.execute_input":"2024-09-03T16:00:20.950217Z","iopub.status.idle":"2024-09-03T16:00:43.992465Z","shell.execute_reply.started":"2024-09-03T16:00:20.950182Z","shell.execute_reply":"2024-09-03T16:00:43.990924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Some additional insights from the EDA**:\n\n* 'clean_title' is either 'True' or was not reported\n* We have high correlations between some features\n* 'fuel_type' is highly imbalanced\n* 'price' is highly skewed \n","metadata":{}},{"cell_type":"markdown","source":"## **Feature Engineering & Preprocessing**","metadata":{}},{"cell_type":"markdown","source":"We now will engineer some useful features for our regression task and fill the NANs. The following code cell will build up several new features (e.g. the number of cylinders) and will fill the NANs and was gratefully taken from https://www.kaggle.com/code/ravi20076/playgrous4e09-baseline-v1:","metadata":{}},{"cell_type":"code","source":"# thanks to https://www.kaggle.com/code/ravi20076/playgrous4e09-baseline-v1 for the following code\nclass FtreMaker:\n    \"This class makes new features using public discussions and kernel ideas\"\n    \n    def __init__(self):\n        self.fuel_type_dict = {\n            'Gasoline': 0,\n            'Hybrid': 1,\n            'E85 Flex Fuel': 2,\n            'uknown': 3,\n            'Diesel': 4,\n            'dash': 5,\n            'Plug-In Hybrid': 6,\n            'not supported': 7\n        }\n\n        self.accident_dict = {\n            'None reported': 0,\n            'At least 1 accident or damage reported': 1,\n            'uknown': 2\n        }\n\n        self.clean_title_dict = {\n            'Yes': 0,\n            'uknown': 1\n        }\n\n        self.expensive_ext_color = \\\n        [  'Blue Caelum', 'Dark Sapphire', 'Bianco Monocerus', 'C / C', 'Ice',\n           'Tempest', 'Beluga Black', 'Bianco Icarus Metallic', \n           'BLU ELEOS', 'Shadow Black', 'Nero Noctis', 'Sandstone Metallic',\n           'Lizard Green', 'Balloon White', 'Onyx', 'Donington Grey Metallic',\n           'China Blue', 'Diamond White', 'Rosso Corsa', 'Granite',\n           'Rosso Mars Metallic', 'Carpathian Grey', 'Kemora Gray Metallic',\n           'Grigio Nimbus', 'dash', 'Bianco Isis', 'Python Green', 'Fountain Blue',\n           'Custom Color', 'Vega Blue', 'Designo Magno Matte',\n           'Brands Hatch Gray Metallic', 'Rift Metallic', 'Gentian Blue Metallic',\n           'Arancio Borealis', 'BLUE', 'Aventurine Green Metallic', 'Apex Blue',\n           'Daytona Gray Pearl Effect', 'Daytona Gray Pearl Effect w/ Black Roof',\n           'Matte White', 'Carpathian Grey Premium Metallic', 'Blue Metallic',\n           'Santorini Black Metallic', 'Quartzite Grey Metallic',\n           'Carrara White Metallic', 'BLACK', 'Kinetic Blue', 'Nero Daytona'\n        ]\n\n        self.expensive_int_color = \\\n        ['Dark Auburn', 'Hotspur', 'Cobalt Blue', 'Beluga Hide', 'Linen',\n         'Beluga', 'Black / Brown', 'Nero Ade', 'Sahara Tan', 'Portland']\n\n        self.expensive_hp = \\\n        [443.0, 473.0, 493.0, 502.0, 521.0, 542.0, \n         543.0, 571.0, 572.0, 573.0, 580.0,\n         591.0, 602.0, 611.0, 616.0, 620.0, 624.0, \n         640.0, 641.0, 651.0, 710.0, 715.0, 760.0, 788.0, 797.0\n        ]\n        \n        self.series_pattern = \\\n        re.compile(r'^[A-Za-z0-9\\-]+')\n        \n        self.version_pattern = \\\n        re.compile(r'([0-9]+\\.[0-9]+[A-Za-z]*)|([A-Z]+[0-9]*)')\n        \n        self.trim_pattern = \\\n        re.compile(r'\\b(Base|Sport|Premium|Ultimate|XLT|LZ|LT|Plus|Touring|SE|LE|Limited|Platinum|Performance|S|V6|GT|EX|SX|XLE|SR|SL|SV|XSE|TRD|RS|GranSport|Signature|Quad Cab|DRW|Cabriolet|Carbon Edition|Trail Boss|Prestige|Essence|Reserve|xDrive|4MATIC|PreRunner|EcoBoost|Scat Pack|Competition|Adventure Package|Laramie|Grand Touring|Long Range)\\b')\n        \n        self.hp_pattern = r'(\\d+(\\.\\d+)?)HP'\n        self.displacement_pattern = r'(\\d+\\.\\d+)L'\n        self.cylinder_pattern = r'(\\d+) Cylinder'\n\n        self.engine_configs = {\n            'V6': 'V6', \n            'V8': 'V8', \n            'V10': 'V10', \n            'V12': 'V12',\n            'Straight 6': 'Straight 6', \n            'Flat 6': 'Flat 6', \n            'I4': 'I4'\n        }\n\n        self.forced_induction = {'Turbo', 'Twin Turbo'}\n        self.valve_configs    = {'DOHC', 'SOHC', 'GDI', 'PDI'}\n        self.fuel_systems     = {'MPFI', 'GDI', 'PDI', 'TFSI'}\n        \n    def fit(self, X: pd.DataFrame, y = None, **params):\n        return self\n    \n    def _make_mdlparts(self, model: str):\n        \"This method extracts model components\"\n        \n        series  = self.series_pattern.search(model)\n        version = self.version_pattern.search(model)\n        trim    = self.trim_pattern.search(model)\n\n        return {\n            'Series'  : series.group(0) if series else \"uknown\",\n            'Version' : version.group(0) if version else \"uknown\",\n            'Trim'    : trim.group(0) if trim else \"uknown\"\n        }\n    \n    def _parse_engine(self, engine_desc):\n        \"This metod processes the engine description feature and returns sub-parts\"\n        \n        int_default_na = -1\n        obj_default_na = \"uknown\"\n        \n        features = {\n            'horsepower': int_default_na, \n            'displacement': int_default_na, \n            'cylinder_count': int_default_na,\n            'engine_configuration':obj_default_na, \n            'forced_induction': obj_default_na,\n            'valve_configuration': obj_default_na, \n            'fuel_system': obj_default_na, \n            'mild_hybrid': False,\n        }\n        \n        patterns = {\n            'horsepower'    : (self.hp_pattern, lambda x: float(x)),\n            'displacement'  : (self.displacement_pattern, lambda x: float(x)),\n            'cylinder_count': (self.cylinder_pattern, lambda x: int(x)),\n        }\n        \n        for feature, (pattern, convert) in patterns.items():\n            match = re.search(pattern, engine_desc)\n            if match:\n                try:\n                    features[feature] = convert(match.group(1))\n                except ValueError:\n                    pass  \n        \n        features['engine_configuration'] = \\\n        next((config for config, name in self.engine_configs.items() if config in engine_desc), \n             obj_default_na\n            )\n        \n        features['forced_induction'] = \\\n        next((induction for induction in self.forced_induction if induction in engine_desc),\n             obj_default_na\n            )\n        \n        features['valve_configuration'] = \\\n        next((valve for valve in self.valve_configs if valve in engine_desc), \n             obj_default_na\n            )\n        \n        features['fuel_system'] = \\\n        next((system for system in self.fuel_systems if system in engine_desc), \n             obj_default_na\n            )\n        \n        features['mild_hybrid'] = 'Mild Electric Hybrid' in engine_desc\n        \n        return features\n    \n    def _get_gear_number(self, trans):\n        match = re.search(r'(\\d+)[-\\s]?Speed', trans)\n        return int(match.group(1)) if match else -1\n\n    def _get_transmission_type(self, trans):\n        if 'A/T' in trans or 'Automatic' in trans:\n            return 'Automatic'\n        elif 'M/T' in trans or 'Manual' in trans:\n            return 'Manual'\n        elif 'CVT' in trans:\n            return 'CVT'\n        elif 'DCT' in trans:\n            return 'DCT'\n        else:\n            return 'Other'\n\n    def _get_special_features(self, trans):\n        features = []\n        if 'Dual Shift Mode' in trans:\n            features.append('Dual Shift Mode')\n        if 'Auto-Shift' in trans:\n            features.append('Auto-Shift')\n        if 'Overdrive' in trans:\n            features.append('Overdrive')\n        if 'Electronically Controlled' in trans:\n            features.append('Electronically Controlled')\n        if 'Variable' in trans:\n            features.append('Variable')\n        return ', '.join(features) if features else \"uknown\"\n\n    def _get_transmission_dsg(self, trans):\n        if 'A/T' in trans:\n            return 'A/T'\n        elif 'M/T' in trans:\n            return 'M/T'\n        elif 'CVT' in trans:\n            return 'CVT'\n        elif 'DCT' in trans:\n            return 'DCT'\n        else:\n            return \"uknown\"\n\n    def transform(self, X: pd.DataFrame, y = None, **params):\n        \"This method makes new features from the existing dataset\"\n\n        df = X.copy()\n        print(f\"\\n\\n---> Starting shape = {df.shape}\")\n        \n        print(f\"---> Imputing category columns\")\n        cat_ftre     = df.select_dtypes(\"object\").columns\n        df[cat_ftre] = df[cat_ftre].fillna(\"uknown\")\n        \n        print(f\"---> Performing -150 feature transforms\")\n        df['fuel_type_encoded']   = df['fuel_type'].map(self.fuel_type_dict)\n        df['accident_encoded']    = df['accident'].map(self.accident_dict)\n        df['clean_title_encoded'] = df['clean_title'].map(self.clean_title_dict)\n        \n        df['expensive_color_ext_encoded'] = \\\n        df.ext_col.isin(self.expensive_ext_color).astype(int)\n        \n        df['expensive_color_int_encoded'] = \\\n        df.int_col.isin(self.expensive_int_color).astype(int)\n        \n        df['expensive_hp'] = \\\n        df['engine'].str.extract(r'(\\d+\\.?\\d*)HP').\\\n        astype(float).isin(self.expensive_hp).\\\n        astype(int)\n        \n        df['cylinder'] = \\\n        df['engine'].str.extract(r'(\\d+\\.?\\d*) Cylinder').astype(float) \n        \n        df['got_V'] = df['model'].str.extract(r'(\\d+\\.?\\d*) V').notna().astype(int)\n        \n        print(f\"---> Making car model components\")\n        mdlparts = df['model'].apply(self._make_mdlparts).apply(pd.Series)\n        df       = pd.concat([df, mdlparts], axis=1)\n        del mdlparts\n        \n        print(f\"---> Making engine components\")\n        unique_engines  = df['engine'].unique()\n        engine_features = [self._parse_engine(engine) for engine in unique_engines]\n        engine_features = pd.DataFrame(engine_features, index = unique_engines).reset_index()\n        engine_features.rename(columns={'index': 'engine'}, inplace=True)\n        \n        df = df.merge(engine_features, how = \"left\", on = \"engine\")\n        del engine_features,unique_engines\n \n        print(f\"---> Making transmission components\")\n        df['nb_gears']   = \\\n        df['transmission'].apply(self._get_gear_number)\n        df['transmission_mode'] = \\\n        df['transmission'].apply(self._get_transmission_type)\n        df['transmission_special']  = \\\n        df['transmission'].apply(self._get_special_features)\n        df['transmission_dsg'] = \\\n        df['transmission'].apply(self._get_transmission_dsg)\n        \n        print(f\"---> Imputing all numeric and categorical features at the end\")\n        num_ftre = df.select_dtypes(include=[np.number]).columns\n        cat_ftre = df.select_dtypes(include=['object']).columns\n        \n        df[num_ftre] = df[num_ftre].fillna(-1)\n        df[cat_ftre] = df[cat_ftre].fillna(\"uknown\").astype(\"category\")\n        \n        #df[\"Source\"] = df[\"Source\"].astype(str)\n  \n        print(f\"\\n---> Final shape = {df.shape}\")\n        return df\n    \nxform = \\\nPipeline(steps = [(\"MakeFtre\", FtreMaker())],verbose = True)\nX_train = df_train.drop(columns=['price'])\ny_train = df_train['price']\nX_test  = df_test.copy()\n\ndf_train = xform.fit_transform(X_train, y_train)\ndf_test  = xform.transform(X_test)\ndf_train['price'] = y_train","metadata":{"execution":{"iopub.status.busy":"2024-09-03T16:00:43.994601Z","iopub.execute_input":"2024-09-03T16:00:43.995162Z","iopub.status.idle":"2024-09-03T16:02:02.905427Z","shell.execute_reply.started":"2024-09-03T16:00:43.995108Z","shell.execute_reply":"2024-09-03T16:02:02.904009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we use target encoding to encode all features. For this, we first define all categorical features and then execute the encoding:","metadata":{}},{"cell_type":"code","source":"cat_features = ['brand', 'model', 'model_year', 'fuel_type', 'engine',\n       'transmission', 'ext_col', 'int_col', 'accident', 'clean_title',\n       'fuel_type_encoded', 'accident_encoded', 'clean_title_encoded',\n       'expensive_color_ext_encoded', 'expensive_color_int_encoded',\n       'expensive_hp', 'cylinder', 'got_V', 'Series', 'Version', 'Trim',\n       'horsepower', 'displacement', 'cylinder_count', 'engine_configuration',\n       'forced_induction', 'valve_configuration', 'fuel_system', 'mild_hybrid',\n       'nb_gears', 'transmission_mode', 'transmission_special',\n       'transmission_dsg']\n\n# we also save the column names for later before encoding the features\ncol_names = df_train.columns","metadata":{"execution":{"iopub.status.busy":"2024-09-03T16:02:02.907222Z","iopub.execute_input":"2024-09-03T16:02:02.908016Z","iopub.status.idle":"2024-09-03T16:02:02.915234Z","shell.execute_reply.started":"2024-09-03T16:02:02.907955Z","shell.execute_reply":"2024-09-03T16:02:02.914024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tar_encoder = TargetEncoder(smooth=\"auto\", target_type='continuous', cv=5, random_state=0)\n\ntar_cols_train = pd.DataFrame(tar_encoder.fit_transform(df_train[cat_features], df_train['price']))\ntar_cols_test = pd.DataFrame(tar_encoder.transform(df_test[cat_features]))\n\n# One-hot encoding removes index, we need to put it back\ntar_cols_train.index = df_train.index\ntar_cols_test.index = df_test.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = df_train.drop(cat_features, axis=1)\nnum_X_test= df_test.drop(cat_features, axis=1)\n\n# Add one-hot encoded columns to numerical features\ndf_train = pd.concat([num_X_train, tar_cols_train], axis=1)\ndf_test = pd.concat([num_X_test, tar_cols_test], axis=1)\n\n# Ensure all columns have string type\ndf_train.columns = df_train.columns.astype(str)\ndf_test.columns = df_test.columns.astype(str)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T16:02:02.91652Z","iopub.execute_input":"2024-09-03T16:02:02.916853Z","iopub.status.idle":"2024-09-03T16:02:07.378743Z","shell.execute_reply.started":"2024-09-03T16:02:02.916819Z","shell.execute_reply":"2024-09-03T16:02:07.37752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's now analyze the most important features for the regression task. A method for this is to use a shallow regression tree (inspired by this https://www.kaggle.com/competitions/playground-series-s4e6/discussion/509073) and we can see that the 'model' feature seems to be the most important feature since it is used in the first levels of the tree:","metadata":{}},{"cell_type":"code","source":"# thanks to https://www.kaggle.com/code/ambrosm/pss4e6-eda-which-makes-sense for (the most parts of) the code\ndt = DecisionTreeRegressor(max_depth=3)\ndt.fit(df_train, df_train.price)\n\nplt.figure(figsize=(16, 6))\nplot_tree(dt, feature_names=col_names, class_names=['price'], fontsize=7, impurity=False, filled=True, ax=plt.gca())\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-03T16:02:07.380223Z","iopub.execute_input":"2024-09-03T16:02:07.380579Z","iopub.status.idle":"2024-09-03T16:02:08.754968Z","shell.execute_reply.started":"2024-09-03T16:02:07.380544Z","shell.execute_reply":"2024-09-03T16:02:08.75362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Modeling**","metadata":{}},{"cell_type":"markdown","source":"We now train our model via Auto ML with the help of the package 'Autogluon'. It is as simple as follows:","metadata":{}},{"cell_type":"code","source":"model = TabularPredictor(label='price',\n                            eval_metric='rmse',\n                            problem_type='regression').fit(df_train,\n                                                       presets='best_quality',\n                                                       time_limit=3600,\n                                                       excluded_model_types=['KNN'])","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-09-03T16:02:08.756473Z","iopub.execute_input":"2024-09-03T16:02:08.756905Z","iopub.status.idle":"2024-09-03T17:09:09.054007Z","shell.execute_reply.started":"2024-09-03T16:02:08.756861Z","shell.execute_reply":"2024-09-03T17:09:09.052758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.leaderboard()","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:09:09.055717Z","iopub.execute_input":"2024-09-03T17:09:09.056085Z","iopub.status.idle":"2024-09-03T17:09:09.083895Z","shell.execute_reply.started":"2024-09-03T17:09:09.056046Z","shell.execute_reply":"2024-09-03T17:09:09.082692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the best model performs well in the validation and increasing the training time can significantly improve the scores further (especially on the leaderboard).\n\nWe can use the best performing model for producing the submission predictions as follows:","metadata":{}},{"cell_type":"code","source":"y_pred = model.predict(df_test)\ndf_sub['price'] = y_pred\ndf_sub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:09:09.086221Z","iopub.execute_input":"2024-09-03T17:09:09.086771Z","iopub.status.idle":"2024-09-03T17:09:57.559388Z","shell.execute_reply.started":"2024-09-03T17:09:09.086718Z","shell.execute_reply":"2024-09-03T17:09:57.558185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub_test = pd.read_csv('submission.csv')\ndf_sub_test.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:09:57.560883Z","iopub.execute_input":"2024-09-03T17:09:57.561317Z","iopub.status.idle":"2024-09-03T17:09:57.603391Z","shell.execute_reply.started":"2024-09-03T17:09:57.561278Z","shell.execute_reply":"2024-09-03T17:09:57.602173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Summary and possible next steps:**\nIn summary, we successfully built an EDA and a model for the PS4E9 task using Auto EDA and Auto ML. Based on this, next steps could be:\n* Engineer additonal useful features\n* Increase model complexity e.g. through longer Auto ML training or by combining the Auto ML model with other models in an ensemble","metadata":{}}]}