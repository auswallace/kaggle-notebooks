{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":125192,"databundleVersionId":15408205,"sourceType":"competition"}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Heart Disease Prediction ‚Äî Ensemble GBDT Approach\n## Playground Series S6E2 | Target: AUC-ROC\n\n**Strategy:**\n- Proper feature typing (continuous vs categorical)\n- Domain-informed feature engineering\n- Tuned XGBoost + LightGBM + CatBoost\n- Blended ensemble for robust predictions\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder\n\nimport xgboost as xgb\nimport lightgbm as lgb\nimport catboost as cb\n\nimport optuna\nfrom optuna.samplers import TPESampler\noptuna.logging.set_verbosity(optuna.logging.WARNING)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.max_columns', 50)\nprint(\"All imports loaded\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T19:16:27.2034Z","iopub.execute_input":"2026-02-04T19:16:27.203688Z","iopub.status.idle":"2026-02-04T19:16:36.201541Z","shell.execute_reply.started":"2026-02-04T19:16:27.203664Z","shell.execute_reply":"2026-02-04T19:16:36.200679Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load data\ntrain = pd.read_csv('/kaggle/input/playground-series-s6e2/train.csv')\ntest = pd.read_csv('/kaggle/input/playground-series-s6e2/test.csv')\nsub = pd.read_csv('/kaggle/input/playground-series-s6e2/sample_submission.csv')\n\nprint(f\"Train shape: {train.shape}\")\nprint(f\"Test shape:  {test.shape}\")\ntrain.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T19:16:53.421891Z","iopub.execute_input":"2026-02-04T19:16:53.422626Z","iopub.status.idle":"2026-02-04T19:16:54.698697Z","shell.execute_reply.started":"2026-02-04T19:16:53.422594Z","shell.execute_reply":"2026-02-04T19:16:54.697689Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Exploratory Data Analysis\n","metadata":{}},{"cell_type":"code","source":"# Target encoding\ntrain['Heart Disease'] = train['Heart Disease'].map({'Absence': 0, 'Presence': 1})\n\nprint(\"Target Distribution:\")\nprint(train['Heart Disease'].value_counts(normalize=True))\nprint(f\"\\nMissing values in train: {train.isnull().sum().sum()}\")\nprint(f\"Missing values in test:  {test.isnull().sum().sum()}\")\nprint(f\"\\nUnique values per column:\")\nprint(train.drop(['id','Heart Disease'], axis=1).nunique().sort_values())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T19:17:13.437645Z","iopub.execute_input":"2026-02-04T19:17:13.438006Z","iopub.status.idle":"2026-02-04T19:17:13.595687Z","shell.execute_reply.started":"2026-02-04T19:17:13.437981Z","shell.execute_reply":"2026-02-04T19:17:13.594673Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, axes = plt.subplots(3, 5, figsize=(20, 12))\naxes = axes.flatten()\n\nfeatures = [c for c in train.columns if c not in ['id', 'Heart Disease']]\nfor i, col in enumerate(features):\n    ax = axes[i]\n    train[train['Heart Disease']==0][col].hist(ax=ax, alpha=0.5, label='No HD', bins=30, density=True)\n    train[train['Heart Disease']==1][col].hist(ax=ax, alpha=0.5, label='HD', bins=30, density=True)\n    ax.set_title(col, fontsize=10)\n    ax.legend(fontsize=7)\n\n# hide unused subplots\nfor j in range(len(features), len(axes)):\n    axes[j].set_visible(False)\n\nplt.suptitle('Feature Distributions by Target', fontsize=14, y=1.01)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T19:17:19.187861Z","iopub.execute_input":"2026-02-04T19:17:19.188231Z","iopub.status.idle":"2026-02-04T19:17:23.176469Z","shell.execute_reply.started":"2026-02-04T19:17:19.188204Z","shell.execute_reply":"2026-02-04T19:17:23.175307Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12, 8))\ncorr = train.drop('id', axis=1).corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\nsns.heatmap(corr, mask=mask, annot=True, fmt='.2f', cmap='RdBu_r', center=0, \n            square=True, linewidths=0.5)\nplt.title('Correlation Matrix', fontsize=14)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T19:17:31.225515Z","iopub.execute_input":"2026-02-04T19:17:31.225818Z","iopub.status.idle":"2026-02-04T19:17:32.141894Z","shell.execute_reply.started":"2026-02-04T19:17:31.225795Z","shell.execute_reply":"2026-02-04T19:17:32.140904Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature Engineering\n\n**Continuous features:** Age, BP, Cholesterol, Max HR, ST depression  \n**Categorical features:** Sex, Chest pain type, FBS over 120, EKG results, Exercise angina, Slope of ST, Number of vessels fluro, Thallium\n\n","metadata":{}},{"cell_type":"code","source":"def feature_engineering(df):\n    df = df.copy()\n    \n    # --- Interaction Features ---\n    df['Age_x_MaxHR'] = df['Age'] * df['Max HR']\n    df['Age_x_STdep'] = df['Age'] * df['ST depression']\n    df['BP_x_Chol'] = df['BP'] * df['Cholesterol']\n    df['MaxHR_x_STdep'] = df['Max HR'] * df['ST depression']\n    df['Age_x_Vessels'] = df['Age'] * df['Number of vessels fluro']\n    df['BP_x_MaxHR'] = df['BP'] * df['Max HR']\n    \n    # --- Ratio Features ---\n    df['Chol_BP_ratio'] = df['Cholesterol'] / (df['BP'] + 1)\n    df['MaxHR_Age_ratio'] = df['Max HR'] / (df['Age'] + 1)\n    df['STdep_MaxHR_ratio'] = df['ST depression'] / (df['Max HR'] + 1)\n    \n    # --- Polynomial Features for key predictors ---\n    df['STdep_sq'] = df['ST depression'] ** 2\n    df['MaxHR_sq'] = df['Max HR'] ** 2\n    df['Age_sq'] = df['Age'] ** 2\n    \n    # --- Domain-informed bins ---\n    df['Age_bin'] = pd.cut(df['Age'], bins=[0, 40, 50, 60, 100], labels=[0,1,2,3]).astype(int)\n    df['BP_category'] = pd.cut(df['BP'], bins=[0, 120, 130, 140, 300], labels=[0,1,2,3]).astype(int)\n    df['Chol_category'] = pd.cut(df['Cholesterol'], bins=[0, 200, 240, 600], labels=[0,1,2]).astype(int)\n    \n    # --- Aggregated risk score (simple) ---\n    df['risk_score'] = (\n        (df['Age'] > 55).astype(int) +\n        (df['BP'] > 130).astype(int) +\n        (df['Cholesterol'] > 240).astype(int) +\n        (df['Max HR'] < 140).astype(int) +\n        (df['ST depression'] > 1).astype(int) +\n        df['Exercise angina'] +\n        (df['Number of vessels fluro'] > 0).astype(int)\n    )\n    \n    return df\n\n# Define categorical columns for GBDT models\nCAT_COLS = ['Sex', 'Chest pain type', 'FBS over 120', 'EKG results', \n            'Exercise angina', 'Slope of ST', 'Number of vessels fluro', 'Thallium',\n            'Age_bin', 'BP_category', 'Chol_category']\n\n# Apply feature engineering\ntrain_fe = feature_engineering(train)\ntest_fe = feature_engineering(test)\n\n# Separate features and target\ntest_ids = test_fe.pop('id')\ntrain_fe.drop('id', axis=1, inplace=True)\ny = train_fe.pop('Heart Disease')\n\nFEATURES = [c for c in train_fe.columns]\nprint(f\"Total features: {len(FEATURES)}\")\nprint(f\"Categorical features: {len(CAT_COLS)}\")\nprint(f\"Numerical features: {len(FEATURES) - len(CAT_COLS)}\")\ntrain_fe.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T19:18:22.403454Z","iopub.execute_input":"2026-02-04T19:18:22.4038Z","iopub.status.idle":"2026-02-04T19:18:22.837308Z","shell.execute_reply.started":"2026-02-04T19:18:22.403773Z","shell.execute_reply":"2026-02-04T19:18:22.836224Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Training ‚Äî Optuna-Tuned GBDT Ensemble\n\nTune each model individually with Optuna, then blend their predictions.\n\n**Models:**\n1. LightGBM\n2. XGBoost  \n3. CatBoost\n","metadata":{}},{"cell_type":"code","source":"N_SPLITS = 10\nSEED = 42\nN_OPTUNA_TRIALS = 50\n\nskf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\nprint(f\"Using {N_SPLITS}-Fold Stratified CV with seed {SEED}\")\nprint(f\"Optuna trials per model: {N_OPTUNA_TRIALS}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T19:18:50.088017Z","iopub.execute_input":"2026-02-04T19:18:50.088739Z","iopub.status.idle":"2026-02-04T19:18:50.094171Z","shell.execute_reply.started":"2026-02-04T19:18:50.08871Z","shell.execute_reply":"2026-02-04T19:18:50.09317Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def lgb_objective(trial):\n    params = {\n        'objective': 'binary',\n        'metric': 'auc',\n        'verbosity': -1,\n        'boosting_type': 'gbdt',\n        'random_state': SEED,\n        'n_estimators': 3000,\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.15, log=True),\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.4, 1.0),\n        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n        'min_split_gain': trial.suggest_float('min_split_gain', 1e-8, 1.0, log=True),\n    }\n    \n    scores = []\n    for fold, (trn_idx, val_idx) in enumerate(skf.split(train_fe, y)):\n        X_trn, X_val = train_fe.iloc[trn_idx], train_fe.iloc[val_idx]\n        y_trn, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n        \n        model = lgb.LGBMClassifier(**params)\n        model.fit(\n            X_trn, y_trn,\n            eval_set=[(X_val, y_val)],\n            callbacks=[lgb.early_stopping(100, verbose=False), lgb.log_evaluation(0)]\n        )\n        preds = model.predict_proba(X_val)[:, 1]\n        scores.append(roc_auc_score(y_val, preds))\n    \n    return np.mean(scores)\n\nprint(\"üîç Tuning LightGBM...\")\nlgb_study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=SEED))\nlgb_study.optimize(lgb_objective, n_trials=N_OPTUNA_TRIALS, show_progress_bar=True)\nprint(f\"\\n‚úÖ Best LightGBM AUC: {lgb_study.best_value:.6f}\")\nprint(f\"Best params: {lgb_study.best_params}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T19:18:52.076845Z","iopub.execute_input":"2026-02-04T19:18:52.077214Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def xgb_objective(trial):\n    params = {\n        'objective': 'binary:logistic',\n        'eval_metric': 'auc',\n        'tree_method': 'hist',\n        'random_state': SEED,\n        'n_estimators': 3000,\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.15, log=True),\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 50),\n        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.4, 1.0),\n        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n        'gamma': trial.suggest_float('gamma', 1e-8, 5.0, log=True),\n    }\n    \n    scores = []\n    for fold, (trn_idx, val_idx) in enumerate(skf.split(train_fe, y)):\n        X_trn, X_val = train_fe.iloc[trn_idx], train_fe.iloc[val_idx]\n        y_trn, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n        \n        model = xgb.XGBClassifier(**params, verbosity=0, early_stopping_rounds=100)\n        model.fit(\n            X_trn, y_trn,\n            eval_set=[(X_val, y_val)],\n            verbose=False\n        )\n        preds = model.predict_proba(X_val)[:, 1]\n        scores.append(roc_auc_score(y_val, preds))\n    \n    return np.mean(scores)\n\nprint(\"üîç Tuning XGBoost...\")\nxgb_study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=SEED))\nxgb_study.optimize(xgb_objective, n_trials=N_OPTUNA_TRIALS, show_progress_bar=True)\nprint(f\"\\n‚úÖ Best XGBoost AUC: {xgb_study.best_value:.6f}\")\nprint(f\"Best params: {xgb_study.best_params}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# For CatBoost, identify categorical feature indices\ncat_indices = [train_fe.columns.get_loc(c) for c in CAT_COLS if c in train_fe.columns]\n\ndef cb_objective(trial):\n    params = {\n        'loss_function': 'Logloss',\n        'eval_metric': 'AUC',\n        'random_seed': SEED,\n        'iterations': 3000,\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.15, log=True),\n        'depth': trial.suggest_int('depth', 3, 10),\n        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-3, 10.0, log=True),\n        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 5.0),\n        'random_strength': trial.suggest_float('random_strength', 1e-3, 10.0, log=True),\n        'border_count': trial.suggest_int('border_count', 32, 255),\n        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 50),\n        'verbose': 0,\n    }\n    \n    scores = []\n    for fold, (trn_idx, val_idx) in enumerate(skf.split(train_fe, y)):\n        X_trn, X_val = train_fe.iloc[trn_idx], train_fe.iloc[val_idx]\n        y_trn, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n        \n        model = cb.CatBoostClassifier(**params)\n        model.fit(\n            X_trn, y_trn,\n            eval_set=(X_val, y_val),\n            cat_features=cat_indices,\n            early_stopping_rounds=100,\n            verbose=0\n        )\n        preds = model.predict_proba(X_val)[:, 1]\n        scores.append(roc_auc_score(y_val, preds))\n    \n    return np.mean(scores)\n\nprint(\"üîç Tuning CatBoost...\")\ncb_study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=SEED))\ncb_study.optimize(cb_objective, n_trials=N_OPTUNA_TRIALS, show_progress_bar=True)\nprint(f\"\\n‚úÖ Best CatBoost AUC: {cb_study.best_value:.6f}\")\nprint(f\"Best params: {cb_study.best_params}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Final Training with Best Params + Ensemble Blend\n\nTrain all 3 models with their best hyperparameters, collect OOF predictions, and find optimal blend weights.\n","metadata":{}},{"cell_type":"code","source":"def train_full_cv(model_type, best_params):\n    \"\"\"Train a model with full CV, return OOF preds and test preds.\"\"\"\n    oof_preds = np.zeros(len(train_fe))\n    test_preds = np.zeros(len(test_fe))\n    fold_scores = []\n    \n    for fold, (trn_idx, val_idx) in enumerate(skf.split(train_fe, y)):\n        X_trn, X_val = train_fe.iloc[trn_idx], train_fe.iloc[val_idx]\n        y_trn, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n        \n        if model_type == 'lgb':\n            params = {\n                'objective': 'binary', 'metric': 'auc', 'verbosity': -1,\n                'boosting_type': 'gbdt', 'random_state': SEED, 'n_estimators': 3000,\n                **best_params\n            }\n            model = lgb.LGBMClassifier(**params)\n            model.fit(X_trn, y_trn, eval_set=[(X_val, y_val)],\n                      callbacks=[lgb.early_stopping(100, verbose=False), lgb.log_evaluation(0)])\n        \n        elif model_type == 'xgb':\n            params = {\n                'objective': 'binary:logistic', 'eval_metric': 'auc',\n                'tree_method': 'hist', 'random_state': SEED, 'n_estimators': 3000,\n                'verbosity': 0, 'early_stopping_rounds': 100, **best_params\n            }\n            model = xgb.XGBClassifier(**params)\n            model.fit(X_trn, y_trn, eval_set=[(X_val, y_val)], verbose=False)\n        \n        elif model_type == 'cb':\n            params = {\n                'loss_function': 'Logloss', 'eval_metric': 'AUC',\n                'random_seed': SEED, 'iterations': 3000, 'verbose': 0,\n                **best_params\n            }\n            model = cb.CatBoostClassifier(**params)\n            model.fit(X_trn, y_trn, eval_set=(X_val, y_val),\n                      cat_features=cat_indices, early_stopping_rounds=100, verbose=0)\n        \n        val_pred = model.predict_proba(X_val)[:, 1]\n        oof_preds[val_idx] = val_pred\n        test_preds += model.predict_proba(test_fe[FEATURES])[:, 1] / N_SPLITS\n        \n        score = roc_auc_score(y_val, val_pred)\n        fold_scores.append(score)\n        print(f\"  Fold {fold+1}: AUC = {score:.6f}\")\n    \n    mean_score = np.mean(fold_scores)\n    print(f\"  ‚û°Ô∏è  Mean AUC: {mean_score:.6f} (¬±{np.std(fold_scores):.6f})\\n\")\n    return oof_preds, test_preds, mean_score\n\n# Train all models\nprint(\"=\" * 60)\nprint(\"üìà LightGBM ‚Äî Final CV Training\")\nprint(\"=\" * 60)\nlgb_oof, lgb_test, lgb_score = train_full_cv('lgb', lgb_study.best_params)\n\nprint(\"=\" * 60)\nprint(\"üìà XGBoost ‚Äî Final CV Training\")\nprint(\"=\" * 60)\nxgb_oof, xgb_test, xgb_score = train_full_cv('xgb', xgb_study.best_params)\n\nprint(\"=\" * 60)\nprint(\"üìà CatBoost ‚Äî Final CV Training\")\nprint(\"=\" * 60)\ncb_oof, cb_test, cb_score = train_full_cv('cb', cb_study.best_params)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from scipy.optimize import minimize\n\ndef blend_objective(weights):\n    w1, w2, w3 = weights\n    blend = w1 * lgb_oof + w2 * xgb_oof + w3 * cb_oof\n    return -roc_auc_score(y, blend)  # negative because we minimize\n\n# Optimize blend weights\nresult = minimize(\n    blend_objective,\n    x0=[1/3, 1/3, 1/3],\n    method='Nelder-Mead',\n    bounds=[(0, 1), (0, 1), (0, 1)]\n)\n\nw1, w2, w3 = result.x\n# Normalize weights\ntotal = w1 + w2 + w3\nw1, w2, w3 = w1/total, w2/total, w3/total\n\nblend_oof = w1 * lgb_oof + w2 * xgb_oof + w3 * cb_oof\nblend_score = roc_auc_score(y, blend_oof)\n\nprint(f\"\\n{'='*60}\")\nprint(f\"üèÜ RESULTS SUMMARY\")\nprint(f\"{'='*60}\")\nprint(f\"LightGBM CV:  {lgb_score:.6f}\")\nprint(f\"XGBoost CV:   {xgb_score:.6f}\")\nprint(f\"CatBoost CV:  {cb_score:.6f}\")\nprint(f\"\\nOptimal Blend Weights: LGB={w1:.3f} | XGB={w2:.3f} | CB={w3:.3f}\")\nprint(f\"\\nüéØ Blended CV AUC: {blend_score:.6f}\")\nprint(f\"\\nüìä Baseline (Logistic Regression): ~0.9500\")\nprint(f\"üìä Our improvement: +{(blend_score - 0.95)*100:.2f}% AUC\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train a single LGB model for feature importance visualization\nimp_model = lgb.LGBMClassifier(\n    objective='binary', metric='auc', verbosity=-1, n_estimators=1000,\n    random_state=SEED, **lgb_study.best_params\n)\nimp_model.fit(train_fe, y)\n\nimportance = pd.DataFrame({\n    'feature': FEATURES,\n    'importance': imp_model.feature_importances_\n}).sort_values('importance', ascending=True).tail(20)\n\nplt.figure(figsize=(10, 8))\nplt.barh(importance['feature'], importance['importance'], color='#2ecc71')\nplt.title('Top 20 Feature Importances (LightGBM)', fontsize=14)\nplt.xlabel('Importance')\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Generate Submission\n","metadata":{}},{"cell_type":"code","source":"# Create blended test predictions\nfinal_preds = w1 * lgb_test + w2 * xgb_test + w3 * cb_test\n\nsubmission = pd.DataFrame({\n    'id': test_ids,\n    'Heart Disease': final_preds\n})\n\nsubmission.to_csv('submission.csv', index=False)\nprint(f\"Submission shape: {submission.shape}\")\nprint(f\"\\nPrediction stats:\")\nprint(submission['Heart Disease'].describe())\nsubmission.head(10)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\naxes[0].hist(blend_oof[y==0], bins=50, alpha=0.6, label='No HD', density=True)\naxes[0].hist(blend_oof[y==1], bins=50, alpha=0.6, label='HD', density=True)\naxes[0].set_title('OOF Prediction Distribution by Target')\naxes[0].legend()\naxes[0].set_xlabel('Predicted Probability')\n\naxes[1].hist(final_preds, bins=50, alpha=0.7, color='#3498db', density=True)\naxes[1].set_title('Test Prediction Distribution')\naxes[1].set_xlabel('Predicted Probability')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Done: submission.csv is ready for upload.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}