{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14491266,"sourceType":"datasetVersion","datasetId":9187454}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ğŸ“± The Instagram Behavioral Genome\n## Decoding 1 Million User Profiles: Where Digital Habits Meet Real Life\n\n**Dataset:** 1M synthetic Instagram user profiles (2025â€“2026) with 57 features spanning demographics, lifestyle, health metrics, and platform engagement.\n\n**Approach:** This notebook is structured as an investigation. Each section poses a question, runs the analysis, and states what the data tells us.\n\n---\n\n### Table of Contents\n1. **Who are these 1M users?** â€” Profiling & demographics\n2. **Where does all the screen time actually go?** â€” Time allocation breakdown\n3. **Can we detect doom-scrollers from their behavioral fingerprint?** â€” Building a composite addiction index\n4. **Does doom-scrolling actually predict worse mental health?** â€” Validating against stress, happiness, sleep\n5. **Is there a tipping point where usage goes from fine to harmful?** â€” Inflection point analysis\n6. **What distinct user personas emerge from the data?** â€” Clustering & radar profiles\n7. **Do healthier lifestyles buffer against compulsive usage?** â€” Health Ã— behavior intersection\n8. **What features most strongly predict someone's happiness?** â€” Feature importance\n","metadata":{}},{"cell_type":"markdown","source":"---\n## âš™ï¸ Environment Setup\n\n- Load all required libraries\n- Define the master light theme palette and helper functions\n- These get reused across every chart in the notebook\n","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport os, glob\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport matplotlib.patheffects as pe\nfrom matplotlib.gridspec import GridSpec\nfrom matplotlib.colors import LinearSegmentedColormap, to_rgba\nfrom matplotlib.patches import FancyBboxPatch\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import gaussian_kde\nfrom scipy.cluster.hierarchy import linkage, dendrogram, fcluster\nfrom scipy.spatial.distance import pdist\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.feature_selection import mutual_info_regression\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# MASTER LIGHT THEME\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nP = {\n    'bg':       '#FFFFFF',\n    'card':     '#F8F9FA',\n    'grid':     '#E9ECEF',\n    'border':   '#DEE2E6',\n    'text':     '#212529',\n    'muted':    '#6C757D',\n    'subtle':   '#ADB5BD',\n\n    # Instagram-inspired accent palette\n    'purple':   '#833AB4',\n    'magenta':  '#C13584',\n    'red':      '#E1306C',\n    'orange':   '#F56040',\n    'gold':     '#FCAD03',\n    'yellow':   '#FFDC80',\n\n    # Functional accents\n    'blue':     '#3A86FF',\n    'cyan':     '#00B4D8',\n    'mint':     '#06D6A0',\n    'pink':     '#EF476F',\n    'coral':    '#FF6B6B',\n    'lavender': '#B8B8FF',\n}\n\nIG_GRAD = [P['purple'], P['magenta'], P['red'], P['orange'], P['gold']]\nACCENT_SEQ = [P['blue'], P['mint'], P['purple'], P['coral'], P['cyan'], P['gold']]\n\nplt.rcParams.update({\n    'figure.facecolor':     P['bg'],\n    'axes.facecolor':       P['card'],\n    'axes.edgecolor':       P['border'],\n    'axes.labelcolor':      P['text'],\n    'text.color':           P['text'],\n    'xtick.color':          P['muted'],\n    'ytick.color':          P['muted'],\n    'grid.color':           P['grid'],\n    'grid.alpha':           0.6,\n    'font.size':            11,\n    'axes.titlesize':       14,\n    'axes.titleweight':     'bold',\n    'figure.titlesize':     18,\n    'figure.titleweight':   'bold',\n    'legend.facecolor':     P['card'],\n    'legend.edgecolor':     P['border'],\n    'legend.fontsize':      9,\n    'savefig.facecolor':    P['bg'],\n    'savefig.edgecolor':    P['bg'],\n    'axes.spines.top':      False,\n    'axes.spines.right':    False,\n})\n\ndef section_footer(fig):\n    fig.text(0.99, 0.01, 'Instagram Behavioral Genome Â· 1M Users',\n             fontsize=7, color=P['subtle'], alpha=0.5,\n             ha='right', va='bottom', style='italic')\n\nprint(\"âœ… Light theme loaded\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T19:52:07.506666Z","iopub.execute_input":"2026-02-02T19:52:07.507036Z","iopub.status.idle":"2026-02-02T19:52:14.105469Z","shell.execute_reply.started":"2026-02-02T19:52:07.507006Z","shell.execute_reply":"2026-02-02T19:52:14.103957Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## ğŸ“¥ Data Ingestion\n\n- Load both CSV files from the Kaggle input directory\n- If they share the same schema, concatenate them into one DataFrame\n- Otherwise, use the larger file\n- Quick shape and memory check\n","metadata":{}},{"cell_type":"code","source":"data_dir = '/kaggle/input/social-media-user-analysis/'\nfiles = sorted(glob.glob(os.path.join(data_dir, '*.csv')))\nprint(f\"ğŸ“‚ Files found: {files}\")\n\ndfs = []\nfor f in files:\n    print(f\"  â³ Loading {os.path.basename(f)}...\")\n    tmp = pd.read_csv(f)\n    print(f\"     â†’ {tmp.shape[0]:,} rows Ã— {tmp.shape[1]} cols\")\n    dfs.append(tmp)\n\nif len(dfs) == 2 and list(dfs[0].columns) == list(dfs[1].columns):\n    df = pd.concat(dfs, ignore_index=True)\n    print(f\"\\nğŸ”— Combined both files (identical schemas)\")\nelif len(dfs) == 2:\n    df = max(dfs, key=len)\n    print(f\"\\nâš ï¸ Different schemas â€” using the larger file\")\nelse:\n    df = dfs[0]\n\nprint(f\"\\nğŸ”¢ Final shape: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\nprint(f\"ğŸ’¾ Memory:  {df.memory_usage(deep=True).sum() / 1e6:.1f} MB\")\nprint(f\"\\nğŸ“‹ Columns:\\n{list(df.columns)}\")\ndf.head(3)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T19:52:14.107685Z","iopub.execute_input":"2026-02-02T19:52:14.108375Z","iopub.status.idle":"2026-02-02T19:53:00.355644Z","shell.execute_reply.started":"2026-02-02T19:52:14.108344Z","shell.execute_reply":"2026-02-02T19:53:00.354814Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Quick structural audit\nnum_cols = df.select_dtypes(include='number').columns.tolist()\ncat_cols = df.select_dtypes(include='object').columns.tolist()\n\nprint(f\"Numeric columns: {len(num_cols)}\")\nprint(f\"Categorical columns: {len(cat_cols)}\")\nprint(f\"\\nNull percentages (top 10):\")\nprint((df.isnull().sum() / len(df) * 100).sort_values(ascending=False).head(10).round(2))\nprint(f\"\\nNumeric describe:\")\ndf[num_cols].describe().round(2).T\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T19:53:00.357147Z","iopub.execute_input":"2026-02-02T19:53:00.357558Z","iopub.status.idle":"2026-02-02T19:53:13.11037Z","shell.execute_reply.started":"2026-02-02T19:53:00.357532Z","shell.execute_reply":"2026-02-02T19:53:13.109057Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n# Question 1: Who are these 1 million users?\n\n**What we're looking at:**\n- Age distribution segmented by generation (Gen Z, Millennial, Gen X, Boomer)\n- Gender split and geographic spread (top countries)\n- Income, education, employment, and urban/rural breakdowns\n- How each demographic segment maps to Instagram engagement\n\n**Why it matters:** Before we can find behavioral patterns, we need to understand the population we're working with. Skewed demographics would bias every downstream insight.\n","metadata":{}},{"cell_type":"code","source":"# â”€â”€ Age Distribution with Generational Segments â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nfig, ax = plt.subplots(figsize=(14, 5.5))\n\nage_data = df['age'].dropna()\nbins = np.arange(age_data.min() - 0.5, age_data.max() + 1.5, 1)\nn, bins_out, patches = ax.hist(age_data, bins=bins, density=True,\n                                alpha=0.75, edgecolor='white', linewidth=0.3)\n\ngen_bands = [\n    (13, 27, 'Gen Z',      P['cyan']),\n    (28, 43, 'Millennial',  P['purple']),\n    (44, 59, 'Gen X',       P['gold']),\n    (60, 80, 'Boomer',      P['coral']),\n]\nfor patch, left in zip(patches, bins_out[:-1]):\n    age_val = left + 0.5\n    for lo, hi, _, col in gen_bands:\n        if lo <= age_val <= hi:\n            patch.set_facecolor(col)\n            break\n\n# KDE overlay\nx_kde = np.linspace(age_data.min(), age_data.max(), 300)\nkde = gaussian_kde(age_data.sample(min(50000, len(age_data)), random_state=42))\nax.plot(x_kde, kde(x_kde), color=P['text'], lw=2.5, alpha=0.7)\n\nfor lo, hi, label, col in gen_bands:\n    mid = (lo + hi) / 2\n    ax.text(mid, ax.get_ylim()[1] * 0.92, label, ha='center', fontsize=12,\n            fontweight='bold', color=col, alpha=0.9)\n    ax.axvline(lo, color=col, ls=':', lw=0.8, alpha=0.4)\n\nax.set_title('Age Distribution Across Generational Segments', pad=15)\nax.set_xlabel('Age')\nax.set_ylabel('Density')\nsection_footer(fig)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T19:53:13.111837Z","iopub.execute_input":"2026-02-02T19:53:13.112218Z","iopub.status.idle":"2026-02-02T19:53:14.064176Z","shell.execute_reply.started":"2026-02-02T19:53:13.112181Z","shell.execute_reply":"2026-02-02T19:53:14.062866Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**ğŸ“Œ Insight:** The age distribution tells us about the population balance. Look for whether Gen Z dominates (which would explain high reels consumption downstream) or if it's more evenly spread. Any sharp drop-offs at certain ages may indicate synthetic generation boundaries.\n","metadata":{}},{"cell_type":"code","source":"# â”€â”€ 6-Panel Demographic Overview â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nfig = plt.figure(figsize=(18, 11))\ngs = GridSpec(2, 3, figure=fig, hspace=0.4, wspace=0.35)\n\n# 1 â€” Gender split\nax1 = fig.add_subplot(gs[0, 0])\nif 'gender' in df.columns:\n    counts = df['gender'].value_counts()\n    colors_g = [P['purple'], P['cyan'], P['gold'], P['coral']]\n    ax1.pie(counts.values, labels=counts.index, autopct='%1.1f%%',\n            colors=colors_g[:len(counts)], startangle=90,\n            textprops={'fontsize': 9}, wedgeprops={'edgecolor': 'white', 'linewidth': 1.5})\n    ax1.set_title('Gender Split', fontsize=12)\n\n# 2 â€” Top 10 countries\nax2 = fig.add_subplot(gs[0, 1])\nif 'country' in df.columns:\n    top_c = df['country'].value_counts().head(10)\n    cmap_c = LinearSegmentedColormap.from_list('', [P['cyan'], P['purple']])\n    colors_c = [cmap_c(i / 9) for i in range(10)]\n    ax2.barh(top_c.index[::-1], top_c.values[::-1],\n             color=colors_c, edgecolor='white', height=0.7)\n    ax2.xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, _: f'{x/1000:.0f}K'))\n    ax2.set_title('Top 10 Countries', fontsize=12)\n\n# 3 â€” Income level\nax3 = fig.add_subplot(gs[0, 2])\nif 'income_level' in df.columns:\n    inc = df['income_level'].value_counts()\n    ax3.bar(inc.index, inc.values, color=[P['mint'], P['blue'], P['purple']][:len(inc)],\n            edgecolor='white', width=0.6)\n    ax3.set_title('Income Level', fontsize=12)\n    ax3.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, _: f'{x/1000:.0f}K'))\n\n# 4 â€” Education\nax4 = fig.add_subplot(gs[1, 0])\nif 'education_level' in df.columns:\n    edu = df['education_level'].value_counts()\n    ax4.barh(edu.index, edu.values,\n             color=[IG_GRAD[i % len(IG_GRAD)] for i in range(len(edu))],\n             edgecolor='white', height=0.6)\n    ax4.xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, _: f'{x/1000:.0f}K'))\n    ax4.set_title('Education Level', fontsize=12)\n\n# 5 â€” Employment\nax5 = fig.add_subplot(gs[1, 1])\nif 'employment_status' in df.columns:\n    emp = df['employment_status'].value_counts()\n    ax5.barh(emp.index, emp.values,\n             color=[ACCENT_SEQ[i % len(ACCENT_SEQ)] for i in range(len(emp))],\n             edgecolor='white', height=0.6)\n    ax5.xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, _: f'{x/1000:.0f}K'))\n    ax5.set_title('Employment Status', fontsize=12)\n\n# 6 â€” Urban / Rural\nax6 = fig.add_subplot(gs[1, 2])\nif 'urban_rural' in df.columns:\n    ur = df['urban_rural'].value_counts()\n    ax6.bar(ur.index, ur.values, color=[P['blue'], P['gold'], P['mint']][:len(ur)],\n            edgecolor='white', width=0.5)\n    ax6.set_title('Urban vs Rural', fontsize=12)\n    ax6.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, _: f'{x/1000:.0f}K'))\n\nfig.suptitle('Demographic Snapshot â€” 1M Instagram Users',\n             fontsize=17, fontweight='bold', color=P['text'], y=1.01)\nsection_footer(fig)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T19:53:14.065612Z","iopub.execute_input":"2026-02-02T19:53:14.066036Z","iopub.status.idle":"2026-02-02T19:53:16.319118Z","shell.execute_reply.started":"2026-02-02T19:53:14.066005Z","shell.execute_reply":"2026-02-02T19:53:16.318105Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**ğŸ“Œ Insight:** Review the demographic panels above for balance. If gender or geography is skewed, we'll need to keep that in mind when interpreting behavioral correlations. Even splits suggest the synthetic generation was well-calibrated.\n","metadata":{}},{"cell_type":"markdown","source":"---\n# Question 2: Where does all the screen time actually go?\n\n**What we're looking at:**\n- How total daily active minutes break down across Instagram's four surfaces: Feed, Explore, Reels, Messages\n- KDE ridge plots for each surface showing distributional shape\n- A stacked composition view: what percentage of each user's time goes where?\n\n**Why it matters:** \"Screen time\" is a blunt metric. A user spending 60 minutes chatting in DMs is fundamentally different from one spending 60 minutes on the Reels feed. This decomposition is the foundation for the doom-scroll index we build next.\n","metadata":{}},{"cell_type":"code","source":"# â”€â”€ Time Allocation Ridge Plot â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ntime_cols = ['time_on_feed_per_day', 'time_on_explore_per_day',\n             'time_on_reels_per_day', 'time_on_messages_per_day',\n             'daily_active_minutes_instagram']\navail_time = [c for c in time_cols if c in df.columns]\n\nfig, axes = plt.subplots(len(avail_time), 1, figsize=(14, 2.2 * len(avail_time)))\n\ncolors_ridge = [P['purple'], P['blue'], P['coral'], P['mint'], P['gold']]\n\nfor i, col in enumerate(avail_time):\n    ax = axes[i]\n    data = df[col].dropna()\n\n    x_range = np.linspace(data.quantile(0.002), data.quantile(0.998), 500)\n    kde = gaussian_kde(data.sample(min(50000, len(data)), random_state=42))\n    density = kde(x_range)\n\n    ax.fill_between(x_range, density, alpha=0.35, color=colors_ridge[i])\n    ax.plot(x_range, density, color=colors_ridge[i], lw=2.5)\n\n    med = data.median()\n    ax.axvline(med, color=P['text'], ls='--', lw=1, alpha=0.5)\n    ax.text(0.98, 0.8, f'median = {med:.1f}   Î¼ = {data.mean():.1f}   Ïƒ = {data.std():.1f}',\n            transform=ax.transAxes, ha='right', fontsize=9,\n            color=P['muted'], family='monospace',\n            bbox=dict(facecolor='white', alpha=0.8, edgecolor=P['grid'], boxstyle='round,pad=0.3'))\n\n    label = col.replace('time_on_', '').replace('_per_day', '').replace('daily_active_minutes_instagram', 'Total Active').replace('_', ' ').title()\n    ax.set_ylabel(label, fontsize=9, rotation=0, labelpad=90, va='center', fontweight='bold',\n                  color=colors_ridge[i])\n    ax.set_yticks([])\n    ax.spines['left'].set_visible(False)\n\naxes[-1].set_xlabel('Minutes per Day')\nfig.suptitle('Time Allocation Across Instagram Surfaces',\n             fontsize=16, fontweight='bold', y=1.02)\nsection_footer(fig)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T19:53:16.320499Z","iopub.execute_input":"2026-02-02T19:53:16.32109Z","iopub.status.idle":"2026-02-02T19:53:20.441475Z","shell.execute_reply.started":"2026-02-02T19:53:16.321055Z","shell.execute_reply":"2026-02-02T19:53:20.440434Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# â”€â”€ Stacked Composition: Where does each user's time go? â”€â”€â”€â”€â”€â”€\nsurface_cols = ['time_on_feed_per_day', 'time_on_explore_per_day',\n                'time_on_reels_per_day', 'time_on_messages_per_day']\navail_surf = [c for c in surface_cols if c in df.columns]\n\nif avail_surf:\n    # Bin by total daily minutes\n    df_temp = df[avail_surf + ['daily_active_minutes_instagram']].dropna().copy()\n    df_temp['usage_bin'] = pd.qcut(df_temp['daily_active_minutes_instagram'], 10,\n                                    labels=[f'D{i+1}' for i in range(10)], duplicates='drop')\n\n    comp = df_temp.groupby('usage_bin')[avail_surf].mean()\n    comp_pct = comp.div(comp.sum(axis=1), axis=0) * 100\n\n    fig, ax = plt.subplots(figsize=(14, 6))\n    colors_stack = [P['purple'], P['blue'], P['coral'], P['mint']]\n    labels_stack = [c.replace('time_on_', '').replace('_per_day', '').replace('_', ' ').title()\n                    for c in avail_surf]\n\n    comp_pct.plot.bar(stacked=True, ax=ax, color=colors_stack[:len(avail_surf)],\n                       edgecolor='white', linewidth=0.5, width=0.8)\n\n    ax.set_ylabel('% of Time')\n    ax.set_xlabel('Usage Decile (D1 = Lightest Users â†’ D10 = Heaviest Users)')\n    ax.set_title('Time Composition Shifts as Total Usage Increases', pad=15)\n    ax.legend(labels_stack, loc='upper left', framealpha=0.9)\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n    ax.set_ylim(0, 100)\n    section_footer(fig)\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T19:53:20.444421Z","iopub.execute_input":"2026-02-02T19:53:20.444687Z","iopub.status.idle":"2026-02-02T19:53:21.506196Z","shell.execute_reply.started":"2026-02-02T19:53:20.444664Z","shell.execute_reply":"2026-02-02T19:53:21.50485Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**ğŸ“Œ Insight:** Pay close attention to how the stacked composition shifts from D1 (lightest users) to D10 (heaviest users). If Reels and Feed dominate for heavy users while Messages stays flat, that confirms the heaviest usage is passive consumption â€” not social interaction. This is the behavioral signature we'll target with the doom-scroll index.\n","metadata":{}},{"cell_type":"markdown","source":"---\n# Question 3: Can we detect doom-scrollers from their behavioral fingerprint?\n\n**What we're looking at:**\n- Building a **Doom-Scroll Index (DSI)** â€” a composite score from 0â€“100 that isolates passive, compulsive consumption\n- High-weight inputs: reels time, feed time, session length, notification response rate\n- Inverse-weight inputs: posts created, comments written, DMs sent (intentional usage lowers the score)\n- Engineered ratios: passive/active ratio, session stickiness, engagement asymmetry\n- Distribution of the DSI across the full population\n\n**Why it matters:** \"Daily active minutes\" treats a content creator and a doom-scroller the same. This index separates passive consumption from active participation, giving us a much sharper lens for the wellbeing analysis that follows.\n","metadata":{}},{"cell_type":"code","source":"# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# DOOM-SCROLL INDEX CONSTRUCTION\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# â”€â”€ Step 1: Engineer ratio features â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndf_dsi = df.copy()\n\n# Passive time (feed + reels + explore)\npassive_cols = ['time_on_feed_per_day', 'time_on_reels_per_day', 'time_on_explore_per_day']\nactive_cols  = ['time_on_messages_per_day', 'posts_created_per_week', 'comments_written_per_day']\n\navail_passive = [c for c in passive_cols if c in df.columns]\navail_active  = [c for c in active_cols if c in df.columns]\n\nif avail_passive:\n    df_dsi['passive_time'] = df_dsi[avail_passive].sum(axis=1)\nif avail_active:\n    df_dsi['active_engagement'] = df_dsi[avail_active].sum(axis=1)\n\n# Passive / Active ratio\nif 'passive_time' in df_dsi.columns and 'active_engagement' in df_dsi.columns:\n    df_dsi['passive_active_ratio'] = df_dsi['passive_time'] / (df_dsi['active_engagement'] + 1)\n\n# Session stickiness = avg session length (long sessions = harder to disengage)\nif 'average_session_length_minutes' in df_dsi.columns:\n    df_dsi['session_stickiness'] = df_dsi['average_session_length_minutes']\nelif 'daily_active_minutes_instagram' in df.columns and 'sessions_per_day' in df.columns:\n    df_dsi['session_stickiness'] = df_dsi['daily_active_minutes_instagram'] / (df_dsi['sessions_per_day'] + 0.1)\n\n# Consumption depth = reels watched per session\nif 'reels_watched_per_day' in df.columns and 'sessions_per_day' in df.columns:\n    df_dsi['consumption_depth'] = df_dsi['reels_watched_per_day'] / (df_dsi['sessions_per_day'] + 0.1)\n\n# Engagement asymmetry = (likes + stories viewed) / (posts + comments + 1)\nasym_consume = [c for c in ['likes_given_per_day', 'stories_viewed_per_day'] if c in df.columns]\nasym_create  = [c for c in ['posts_created_per_week', 'comments_written_per_day'] if c in df.columns]\nif asym_consume and asym_create:\n    df_dsi['engagement_asymmetry'] = df_dsi[asym_consume].sum(axis=1) / (df_dsi[asym_create].sum(axis=1) + 1)\n\nprint(\"âœ… Engineered features created\")\nprint(f\"   passive_active_ratio   â€” mean: {df_dsi['passive_active_ratio'].mean():.2f}\" if 'passive_active_ratio' in df_dsi.columns else \"\")\nprint(f\"   session_stickiness     â€” mean: {df_dsi['session_stickiness'].mean():.2f}\" if 'session_stickiness' in df_dsi.columns else \"\")\nprint(f\"   consumption_depth      â€” mean: {df_dsi['consumption_depth'].mean():.2f}\" if 'consumption_depth' in df_dsi.columns else \"\")\nprint(f\"   engagement_asymmetry   â€” mean: {df_dsi['engagement_asymmetry'].mean():.2f}\" if 'engagement_asymmetry' in df_dsi.columns else \"\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T19:53:21.507508Z","iopub.execute_input":"2026-02-02T19:53:21.508026Z","iopub.status.idle":"2026-02-02T19:53:26.807434Z","shell.execute_reply.started":"2026-02-02T19:53:21.507986Z","shell.execute_reply":"2026-02-02T19:53:26.806161Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# â”€â”€ Step 2: Build the composite index â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n# Select DSI input features (all positive = more doom-scrolly)\ndsi_features = []\ndsi_candidates = [\n    'passive_time', 'passive_active_ratio', 'session_stickiness',\n    'consumption_depth', 'engagement_asymmetry',\n    'reels_watched_per_day', 'stories_viewed_per_day',\n    'notification_response_rate'\n]\ndsi_features = [c for c in dsi_candidates if c in df_dsi.columns]\n\nprint(f\"DSI input features ({len(dsi_features)}):\")\nfor f in dsi_features:\n    print(f\"  â€¢ {f}\")\n\n# Normalize each to 0â€“1 using robust percentile clipping\ndsi_data = df_dsi[dsi_features].copy()\nfor col in dsi_features:\n    lo, hi = dsi_data[col].quantile(0.01), dsi_data[col].quantile(0.99)\n    dsi_data[col] = ((dsi_data[col].clip(lo, hi) - lo) / (hi - lo + 1e-9))\n\n# Weighted sum â†’ scale to 0â€“100\nweights = {\n    'passive_time': 2.0,\n    'passive_active_ratio': 2.0,\n    'session_stickiness': 1.5,\n    'consumption_depth': 1.5,\n    'engagement_asymmetry': 1.5,\n    'reels_watched_per_day': 1.0,\n    'stories_viewed_per_day': 0.8,\n    'notification_response_rate': 1.0,\n}\nw_vec = np.array([weights.get(f, 1.0) for f in dsi_features])\nw_vec = w_vec / w_vec.sum()  # normalize weights to sum=1\n\ndf_dsi['doom_scroll_index'] = (dsi_data[dsi_features].values @ w_vec) * 100\n\n# Also add to main df for downstream use\ndf['doom_scroll_index'] = df_dsi['doom_scroll_index']\n\nprint(f\"\\nğŸ¯ Doom-Scroll Index computed!\")\nprint(f\"   Range:  {df['doom_scroll_index'].min():.1f} â€“ {df['doom_scroll_index'].max():.1f}\")\nprint(f\"   Mean:   {df['doom_scroll_index'].mean():.1f}\")\nprint(f\"   Median: {df['doom_scroll_index'].median():.1f}\")\nprint(f\"   Std:    {df['doom_scroll_index'].std():.1f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T19:53:26.809312Z","iopub.execute_input":"2026-02-02T19:53:26.809895Z","iopub.status.idle":"2026-02-02T19:53:28.589246Z","shell.execute_reply.started":"2026-02-02T19:53:26.809859Z","shell.execute_reply":"2026-02-02T19:53:28.587805Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# â”€â”€ DSI Distribution â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nfig, axes = plt.subplots(1, 2, figsize=(16, 5.5))\n\n# Left: Histogram + KDE\nax = axes[0]\ndsi = df['doom_scroll_index'].dropna()\nn, bins, patches = ax.hist(dsi, bins=60, density=True, alpha=0.7, edgecolor='white', linewidth=0.3)\n\n# Color gradient: green â†’ yellow â†’ red\nfor patch, left_edge in zip(patches, bins[:-1]):\n    frac = (left_edge - bins[0]) / (bins[-1] - bins[0])\n    if frac < 0.4:\n        color = P['mint']\n    elif frac < 0.7:\n        color = P['gold']\n    else:\n        color = P['coral']\n    patch.set_facecolor(color)\n    patch.set_alpha(0.7)\n\nx_kde = np.linspace(dsi.min(), dsi.max(), 300)\nkde = gaussian_kde(dsi.sample(min(50000, len(dsi)), random_state=42))\nax.plot(x_kde, kde(x_kde), color=P['text'], lw=2.5, alpha=0.7)\n\n# Threshold lines\np75 = dsi.quantile(0.75)\np90 = dsi.quantile(0.90)\nax.axvline(p75, color=P['orange'], ls='--', lw=1.5, label=f'75th pctl = {p75:.0f}')\nax.axvline(p90, color=P['red'], ls='--', lw=1.5, label=f'90th pctl = {p90:.0f}')\nax.legend(fontsize=9)\nax.set_xlabel('Doom-Scroll Index (0â€“100)')\nax.set_ylabel('Density')\nax.set_title('Distribution of the Doom-Scroll Index', pad=10)\n\n# Right: DSI by age group\nax2 = axes[1]\nage_bins_def = [(13, 22, '13â€“22'), (23, 30, '23â€“30'), (31, 40, '31â€“40'),\n                (41, 50, '41â€“50'), (51, 60, '51â€“60'), (61, 80, '61â€“80')]\nage_groups = []\nfor lo, hi, label in age_bins_def:\n    subset = df[(df['age'] >= lo) & (df['age'] <= hi)]['doom_scroll_index'].dropna()\n    if len(subset) > 0:\n        age_groups.append((label, subset))\n\nbp = ax2.boxplot([s for _, s in age_groups], labels=[l for l, _ in age_groups],\n                  patch_artist=True, widths=0.6,\n                  medianprops=dict(color=P['text'], lw=2),\n                  whiskerprops=dict(color=P['muted']),\n                  capprops=dict(color=P['muted']),\n                  flierprops=dict(marker='.', markersize=1, alpha=0.2))\nfor i, patch in enumerate(bp['boxes']):\n    patch.set_facecolor(IG_GRAD[i % len(IG_GRAD)])\n    patch.set_alpha(0.6)\n    patch.set_edgecolor(P['border'])\n\nax2.set_xlabel('Age Group')\nax2.set_ylabel('Doom-Scroll Index')\nax2.set_title('DSI by Age Group', pad=10)\nax2.grid(True, axis='y', alpha=0.3)\n\nfig.suptitle('The Doom-Scroll Index â€” Population Overview',\n             fontsize=16, fontweight='bold', y=1.02)\nsection_footer(fig)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T19:53:28.590417Z","iopub.execute_input":"2026-02-02T19:53:28.590801Z","iopub.status.idle":"2026-02-02T19:53:32.42464Z","shell.execute_reply.started":"2026-02-02T19:53:28.590775Z","shell.execute_reply":"2026-02-02T19:53:32.42338Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**ğŸ“Œ Insight:** The DSI distribution reveals what fraction of users fall into concerning usage patterns. The gradient coloring (green â†’ yellow â†’ red) makes it intuitive. The age-group boxplots test the first hypothesis: younger users should have higher DSI scores, confirming the generational compulsive-usage gap. Look at the medians and the spread of outliers.\n","metadata":{}},{"cell_type":"code","source":"# â”€â”€ DSI Component Breakdown: What drives the score? â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nfig, ax = plt.subplots(figsize=(14, 6))\n\n# Correlation of each component with the final DSI\ndsi_corrs = df_dsi[dsi_features + ['doom_scroll_index']].corr()['doom_scroll_index'].drop('doom_scroll_index')\ndsi_corrs = dsi_corrs.sort_values()\n\ncolors_bar = [P['coral'] if v > 0.5 else P['gold'] if v > 0.3 else P['mint'] for v in dsi_corrs.values]\nbars = ax.barh(dsi_corrs.index, dsi_corrs.values, color=colors_bar, edgecolor='white', height=0.6)\n\nfor bar, val in zip(bars, dsi_corrs.values):\n    ax.text(val + 0.01, bar.get_y() + bar.get_height()/2,\n            f'{val:.3f}', va='center', fontsize=9, color=P['text'])\n\nax.set_xlabel('Correlation with Doom-Scroll Index')\nax.set_title('What Drives the DSI? â€” Component Correlations', pad=15)\nax.grid(True, axis='x', alpha=0.3)\nsection_footer(fig)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T19:53:32.426098Z","iopub.execute_input":"2026-02-02T19:53:32.426559Z","iopub.status.idle":"2026-02-02T19:53:33.638598Z","shell.execute_reply.started":"2026-02-02T19:53:32.426505Z","shell.execute_reply":"2026-02-02T19:53:33.637642Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**ğŸ“Œ Insight:** This chart reveals which behavioral signals contribute most to the doom-scroll index. If `passive_active_ratio` and `session_stickiness` dominate, it confirms the index is capturing what we intended â€” users who consume passively for long stretches without creating or engaging.\n","metadata":{}},{"cell_type":"markdown","source":"---\n# Question 4: Does doom-scrolling actually predict worse mental health?\n\n**What we're looking at:**\n- Doom-Scroll Index vs perceived stress â€” is there a clear positive relationship?\n- DSI vs self-reported happiness â€” does passive consumption track with lower happiness?\n- DSI vs sleep hours â€” does doom-scrolling eat into sleep?\n- Hexbin density maps with trend lines and Pearson r values for statistical rigor\n\n**Why it matters:** This is the central question. Building an index is only useful if it reveals something the raw \"minutes\" metric doesn't. We need to see whether DSI is a better predictor of wellbeing than total screen time alone.\n","metadata":{}},{"cell_type":"code","source":"# â”€â”€ DSI vs Wellbeing: Triple Hexbin Panel â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nwellbeing_pairs = [\n    ('doom_scroll_index', 'perceived_stress_score',    'DSI â†’ Perceived Stress'),\n    ('doom_scroll_index', 'self_reported_happiness',    'DSI â†’ Self-Reported Happiness'),\n    ('doom_scroll_index', 'sleep_hours_per_night',      'DSI â†’ Sleep Hours'),\n]\navail_pairs = [(x, y, t) for x, y, t in wellbeing_pairs if x in df.columns and y in df.columns]\n\nfig, axes = plt.subplots(1, len(avail_pairs), figsize=(6.5 * len(avail_pairs), 5.5))\nif len(avail_pairs) == 1:\n    axes = [axes]\n\ncmaps = [\n    LinearSegmentedColormap.from_list('', ['#F8F9FA', P['purple'], P['red']]),\n    LinearSegmentedColormap.from_list('', ['#F8F9FA', P['blue'], P['mint']]),\n    LinearSegmentedColormap.from_list('', ['#F8F9FA', P['cyan'], P['coral']]),\n]\n\nfor idx, (xc, yc, title) in enumerate(avail_pairs):\n    ax = axes[idx]\n    sample = df[[xc, yc]].dropna().sample(min(80000, len(df)), random_state=42)\n\n    hb = ax.hexbin(sample[xc], sample[yc], gridsize=40,\n                    cmap=cmaps[idx], mincnt=1, edgecolors='none')\n    fig.colorbar(hb, ax=ax, shrink=0.7, label='Count')\n\n    # Trend line\n    z = np.polyfit(sample[xc], sample[yc], 1)\n    p_line = np.poly1d(z)\n    x_line = np.linspace(sample[xc].min(), sample[xc].max(), 100)\n    ax.plot(x_line, p_line(x_line), color=P['text'], lw=2.5, ls='--', alpha=0.8)\n\n    r, pval = stats.pearsonr(sample[xc], sample[yc])\n    direction = 'â†—' if r > 0 else 'â†˜'\n    ax.text(0.05, 0.95, f'{direction} r = {r:.3f}   p = {pval:.1e}',\n            transform=ax.transAxes, fontsize=10, va='top',\n            color=P['text'], family='monospace', fontweight='bold',\n            bbox=dict(facecolor='white', alpha=0.85, edgecolor=P['grid'], boxstyle='round,pad=0.4'))\n\n    ax.set_xlabel('Doom-Scroll Index', fontsize=10)\n    ax.set_ylabel(yc.replace('_', ' ').title(), fontsize=10)\n    ax.set_title(title, fontsize=12, fontweight='bold')\n\nfig.suptitle('Does Doom-Scrolling Predict Worse Wellbeing?',\n             fontsize=16, fontweight='bold', y=1.03)\nsection_footer(fig)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T19:53:33.639896Z","iopub.execute_input":"2026-02-02T19:53:33.640561Z","iopub.status.idle":"2026-02-02T19:53:35.095537Z","shell.execute_reply.started":"2026-02-02T19:53:33.640519Z","shell.execute_reply":"2026-02-02T19:53:35.094086Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**ğŸ“Œ Insight:** Look at the Pearson r values and the direction arrows. A positive r between DSI and stress means doom-scrollers report more stress. A negative r between DSI and happiness/sleep means they're less happy and sleeping less. Compare these r values to what you'd get from raw `daily_active_minutes_instagram` â€” if DSI produces stronger correlations, the composite index is capturing something real beyond just \"more time on phone.\"\n","metadata":{}},{"cell_type":"code","source":"# â”€â”€ Comparison: DSI vs raw minutes as predictors â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ncompare_targets = ['perceived_stress_score', 'self_reported_happiness', 'sleep_hours_per_night']\ncompare_predictors = ['doom_scroll_index', 'daily_active_minutes_instagram']\navail_tgts = [c for c in compare_targets if c in df.columns]\navail_preds = [c for c in compare_predictors if c in df.columns]\n\nif avail_tgts and len(avail_preds) == 2:\n    results = []\n    for tgt in avail_tgts:\n        for pred in avail_preds:\n            valid = df[[pred, tgt]].dropna()\n            r, p = stats.pearsonr(valid[pred], valid[tgt])\n            results.append({'Target': tgt, 'Predictor': pred, 'r': r, '|r|': abs(r), 'p-value': p})\n\n    comp_df = pd.DataFrame(results)\n\n    fig, ax = plt.subplots(figsize=(12, 5))\n    x_labels = comp_df['Target'].unique()\n    x_pos = np.arange(len(x_labels))\n    width = 0.35\n\n    dsi_vals = comp_df[comp_df['Predictor'] == 'doom_scroll_index']['|r|'].values\n    raw_vals = comp_df[comp_df['Predictor'] == 'daily_active_minutes_instagram']['|r|'].values\n\n    bars1 = ax.bar(x_pos - width/2, dsi_vals, width, label='Doom-Scroll Index',\n                    color=P['purple'], edgecolor='white', alpha=0.8)\n    bars2 = ax.bar(x_pos + width/2, raw_vals, width, label='Raw Daily Minutes',\n                    color=P['subtle'], edgecolor='white', alpha=0.8)\n\n    for bar, val in zip(list(bars1) + list(bars2), list(dsi_vals) + list(raw_vals)):\n        ax.text(bar.get_x() + bar.get_width()/2, val + 0.003,\n                f'{val:.3f}', ha='center', fontsize=9, fontweight='bold')\n\n    ax.set_xticks(x_pos)\n    ax.set_xticklabels([t.replace('_', ' ').title() for t in x_labels], fontsize=10)\n    ax.set_ylabel('|Pearson r|')\n    ax.set_title('DSI vs Raw Minutes â€” Which Predicts Wellbeing Better?', pad=15)\n    ax.legend(fontsize=10)\n    ax.grid(True, axis='y', alpha=0.3)\n    section_footer(fig)\n    plt.tight_layout()\n    plt.show()\n\n    print(\"\\nğŸ“‹ Full comparison table:\")\n    display(comp_df.round(4))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T19:53:35.097125Z","iopub.execute_input":"2026-02-02T19:53:35.097982Z","iopub.status.idle":"2026-02-02T19:53:36.25477Z","shell.execute_reply.started":"2026-02-02T19:53:35.097937Z","shell.execute_reply":"2026-02-02T19:53:36.253902Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**ğŸ“Œ Insight:** This is the money chart. If the purple bars (DSI) are taller than the gray bars (raw minutes) for stress, happiness, and sleep, it proves the doom-scroll index captures compulsive behavior better than blunt screen time. The magnitude of the gap tells us how much signal we gained by decomposing usage into passive vs active components.\n","metadata":{}},{"cell_type":"markdown","source":"---\n# Question 5: Is there a tipping point where usage goes from fine to harmful?\n\n**What we're looking at:**\n- Binning the DSI into 20 quantiles and plotting mean stress, happiness, and sleep per bin\n- Looking for **inflection points** where the curves bend â€” the threshold between \"normal use\" and \"compulsive\"\n- Segmented regression / visual change-point detection\n\n**Why it matters:** A linear relationship says \"more is worse.\" A tipping point says \"it's fine until X, then it falls off a cliff.\" The latter is far more actionable â€” it defines a boundary.\n","metadata":{}},{"cell_type":"code","source":"# â”€â”€ Tipping Point Curves â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ntargets_tp = ['perceived_stress_score', 'self_reported_happiness', 'sleep_hours_per_night']\navail_tp = [c for c in targets_tp if c in df.columns]\n\ndf_tp = df[['doom_scroll_index'] + avail_tp].dropna().copy()\ndf_tp['dsi_bin'] = pd.qcut(df_tp['doom_scroll_index'], 20, labels=False, duplicates='drop')\n\nbin_stats = df_tp.groupby('dsi_bin').agg(\n    dsi_mean=('doom_scroll_index', 'mean'),\n    **{f'{c}_mean': (c, 'mean') for c in avail_tp},\n    **{f'{c}_sem': (c, 'sem') for c in avail_tp},\n).reset_index()\n\nfig, axes = plt.subplots(1, len(avail_tp), figsize=(6.5 * len(avail_tp), 5.5))\nif len(avail_tp) == 1:\n    axes = [axes]\n\ncolors_tp = [P['coral'], P['mint'], P['blue']]\nlabels_tp = ['Higher = More Stressed', 'Higher = Happier', 'Higher = More Sleep']\n\nfor i, col in enumerate(avail_tp):\n    ax = axes[i]\n    x = bin_stats['dsi_mean']\n    y = bin_stats[f'{col}_mean']\n    yerr = bin_stats[f'{col}_sem'] * 1.96\n\n    ax.fill_between(x, y - yerr, y + yerr, alpha=0.15, color=colors_tp[i])\n    ax.plot(x, y, 'o-', color=colors_tp[i], lw=2.5, markersize=6, markeredgecolor='white', markeredgewidth=1.5)\n\n    # Find steepest change point\n    diffs = np.abs(np.diff(y))\n    if len(diffs) > 0:\n        steepest_idx = np.argmax(diffs) + 1\n        ax.axvline(x.iloc[steepest_idx], color=P['text'], ls=':', lw=1.5, alpha=0.5)\n        ax.annotate(f'Steepest change\\nDSI â‰ˆ {x.iloc[steepest_idx]:.0f}',\n                    xy=(x.iloc[steepest_idx], y.iloc[steepest_idx]),\n                    xytext=(x.iloc[steepest_idx] + 5, y.iloc[steepest_idx]),\n                    fontsize=8, color=P['text'],\n                    arrowprops=dict(arrowstyle='->', color=P['text'], lw=1.2),\n                    bbox=dict(facecolor='white', edgecolor=P['grid'], boxstyle='round,pad=0.3'))\n\n    ax.set_xlabel('Doom-Scroll Index', fontsize=10)\n    ax.set_ylabel(col.replace('_', ' ').title(), fontsize=10)\n    ax.set_title(labels_tp[i], fontsize=11, color=colors_tp[i], fontweight='bold')\n    ax.grid(True, alpha=0.3)\n\nfig.suptitle('Searching for the Tipping Point â€” Wellbeing vs DSI Quantiles',\n             fontsize=16, fontweight='bold', y=1.03)\nsection_footer(fig)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T19:53:36.256017Z","iopub.execute_input":"2026-02-02T19:53:36.256588Z","iopub.status.idle":"2026-02-02T19:53:37.587885Z","shell.execute_reply.started":"2026-02-02T19:53:36.256558Z","shell.execute_reply":"2026-02-02T19:53:37.586839Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**ğŸ“Œ Insight:** The annotations mark where each curve bends most sharply. If stress spikes or happiness plummets at a specific DSI value, that's our tipping point. The 95% confidence bands (shaded regions) tell us whether the bend is statistically meaningful or just noise. A narrow band at the inflection point = high confidence.\n","metadata":{}},{"cell_type":"markdown","source":"---\n# Question 6: What distinct user personas emerge from the data?\n\n**What we're looking at:**\n- KMeans clustering on behavioral + health features to discover natural user archetypes\n- Elbow method to pick optimal K\n- PCA 2D projection to visualize cluster separation\n- Radar profiles for each cluster â€” their behavioral \"fingerprint\"\n- Persona labeling based on dominant traits\n\n**Why it matters:** Not all users are the same. Identifying archetypes (doom-scroller, creator, lurker, power user, balanced) helps us understand which groups are at risk and which are thriving.\n","metadata":{}},{"cell_type":"code","source":"# â”€â”€ Feature Selection & Clustering â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ncluster_features = [\n    'daily_active_minutes_instagram', 'sessions_per_day',\n    'posts_created_per_week', 'reels_watched_per_day',\n    'stories_viewed_per_day', 'likes_given_per_day',\n    'comments_written_per_day', 'followers_count',\n    'time_on_feed_per_day', 'time_on_reels_per_day',\n    'time_on_messages_per_day',\n    'perceived_stress_score', 'self_reported_happiness',\n    'sleep_hours_per_night', 'age', 'doom_scroll_index'\n]\navail_cf = [c for c in cluster_features if c in df.columns]\n\nsample_size = 30000\nsample_df = df[avail_cf].dropna().sample(sample_size, random_state=42).copy()\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(sample_df)\n\n# Elbow method\ninertias = []\nK_range = range(2, 9)\nfor k in K_range:\n    km = KMeans(n_clusters=k, n_init=10, random_state=42, max_iter=200)\n    km.fit(X_scaled)\n    inertias.append(km.inertia_)\n\nfig, ax = plt.subplots(figsize=(10, 4.5))\nax.plot(K_range, inertias, 'o-', color=P['purple'], lw=2.5, markersize=8,\n        markeredgecolor='white', markeredgewidth=2)\nax.fill_between(K_range, inertias, alpha=0.1, color=P['purple'])\nax.set_xlabel('Number of Clusters (K)')\nax.set_ylabel('Inertia')\nax.set_title('Elbow Method â€” Choosing Optimal K', pad=10)\nax.grid(True, alpha=0.3)\nsection_footer(fig)\nplt.tight_layout()\nplt.show()\n\nK_CHOSEN = 5\nkm_final = KMeans(n_clusters=K_CHOSEN, n_init=20, random_state=42)\nlabels = km_final.fit_predict(X_scaled)\nsample_df['cluster'] = labels\nprint(f\"\\nâœ… KMeans fitted with K={K_CHOSEN}\")\nprint(sample_df['cluster'].value_counts().sort_index().to_string())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T19:53:37.589257Z","iopub.execute_input":"2026-02-02T19:53:37.589738Z","iopub.status.idle":"2026-02-02T19:53:49.942174Z","shell.execute_reply.started":"2026-02-02T19:53:37.589686Z","shell.execute_reply":"2026-02-02T19:53:49.94143Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# â”€â”€ PCA Scatter â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\npca = PCA(n_components=2, random_state=42)\nX_pca = pca.fit_transform(X_scaled)\n\nfig, ax = plt.subplots(figsize=(12, 9))\ncluster_colors = [P['cyan'], P['coral'], P['mint'], P['gold'], P['purple'], P['blue']]\n\nfor c in range(K_CHOSEN):\n    mask = labels == c\n    ax.scatter(X_pca[mask, 0], X_pca[mask, 1],\n               c=cluster_colors[c], s=6, alpha=0.3, label=f'Cluster {c}')\n    cx, cy = X_pca[mask, 0].mean(), X_pca[mask, 1].mean()\n    ax.scatter(cx, cy, c=cluster_colors[c], s=250, marker='D',\n               edgecolors=P['text'], linewidths=2, zorder=5)\n    ax.annotate(f'C{c}', (cx, cy), fontsize=12, fontweight='bold',\n                color=P['text'], ha='center', va='center', zorder=6)\n\nax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)')\nax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)')\nax.set_title('User Clusters in PCA Space', fontsize=14, pad=15)\nax.legend(loc='upper right', framealpha=0.9, markerscale=3)\nax.grid(True, alpha=0.15)\nsection_footer(fig)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T19:53:49.943079Z","iopub.execute_input":"2026-02-02T19:53:49.943375Z","iopub.status.idle":"2026-02-02T19:53:50.497306Z","shell.execute_reply.started":"2026-02-02T19:53:49.943348Z","shell.execute_reply":"2026-02-02T19:53:50.496032Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# â”€â”€ Radar Profiles per Cluster â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nradar_cols = [c for c in avail_cf if c not in ['age', 'doom_scroll_index']][:8]\n\ncluster_means = sample_df.groupby('cluster')[radar_cols].mean()\nradar_normed = (cluster_means - cluster_means.min()) / (cluster_means.max() - cluster_means.min() + 1e-9)\n\nangles = np.linspace(0, 2 * np.pi, len(radar_cols), endpoint=False).tolist()\nangles += angles[:1]\n\nfig, axes = plt.subplots(1, K_CHOSEN, figsize=(4.2 * K_CHOSEN, 5),\n                          subplot_kw=dict(polar=True))\n\npersona_names = ['ğŸŒ™ Night Scroller', 'ğŸ“¸ Content Creator',\n                 'ğŸ‘€ Passive Lurker', 'âš¡ Power User', 'ğŸ§˜ Balanced User']\n\nfor c in range(K_CHOSEN):\n    ax = axes[c] if K_CHOSEN > 1 else axes\n    values = radar_normed.iloc[c].values.tolist() + [radar_normed.iloc[c].values[0]]\n\n    ax.fill(angles, values, color=cluster_colors[c], alpha=0.2)\n    ax.plot(angles, values, color=cluster_colors[c], lw=2.5)\n    ax.scatter(angles[:-1], values[:-1], color=cluster_colors[c], s=30, zorder=5)\n\n    short_labels = [c.replace('_per_day', '').replace('_per_week', '')\n                     .replace('_', ' ').title()[:18] for c in radar_cols]\n    ax.set_xticks(angles[:-1])\n    ax.set_xticklabels(short_labels, fontsize=5.5, color=P['muted'])\n    ax.set_ylim(0, 1.1)\n    ax.set_yticks([0.25, 0.5, 0.75, 1.0])\n    ax.set_yticklabels(['', '', '', ''], fontsize=6)\n    ax.grid(color=P['grid'], alpha=0.5)\n    ax.set_facecolor('#FAFAFA')\n    name = persona_names[c] if c < len(persona_names) else f'Cluster {c}'\n    ax.set_title(name, fontsize=10, color=cluster_colors[c], pad=15, fontweight='bold')\n\nfig.suptitle('Cluster Behavioral Fingerprints',\n             fontsize=16, fontweight='bold', y=1.06)\nsection_footer(fig)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T19:53:50.499235Z","iopub.execute_input":"2026-02-02T19:53:50.499653Z","iopub.status.idle":"2026-02-02T19:53:51.584444Z","shell.execute_reply.started":"2026-02-02T19:53:50.499611Z","shell.execute_reply":"2026-02-02T19:53:51.5833Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# â”€â”€ Cluster DSI + Wellbeing Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nsummary_cols = ['doom_scroll_index', 'perceived_stress_score',\n                'self_reported_happiness', 'sleep_hours_per_night',\n                'daily_active_minutes_instagram', 'posts_created_per_week',\n                'reels_watched_per_day', 'age']\navail_summary = [c for c in summary_cols if c in sample_df.columns]\n\ncluster_summary = sample_df.groupby('cluster')[avail_summary].mean().round(2)\nprint(\"ğŸ“‹ Cluster Averages:\")\ndisplay(cluster_summary.T)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T19:53:51.585949Z","iopub.execute_input":"2026-02-02T19:53:51.586338Z","iopub.status.idle":"2026-02-02T19:53:51.610839Z","shell.execute_reply.started":"2026-02-02T19:53:51.586298Z","shell.execute_reply":"2026-02-02T19:53:51.609204Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**ğŸ“Œ Insight:** Each radar chart shows a cluster's behavioral \"DNA.\" Look for:\n- **The doom-scroller cluster** â€” high reels, high feed time, low posts created, high DSI\n- **The content creator** â€” high posts, high followers, moderate total time\n- **The balanced user** â€” moderate everything, good happiness, good sleep\n- **The lurker** â€” low engagement across the board, short sessions\n\nThe summary table below the radars gives exact numbers. Check which cluster has the highest stress and lowest happiness â€” that's your at-risk group.\n","metadata":{}},{"cell_type":"markdown","source":"---\n# Question 7: Do healthier lifestyles act as a buffer against compulsive usage?\n\n**What we're looking at:**\n- Exercise hours, sleep quality, daily steps, and BMI vs doom-scroll index\n- Hexbin density maps with binned-mean overlays for each health metric\n- Testing whether users who exercise more / sleep better have lower DSI even at the same age\n\n**Why it matters:** If healthy behaviors correlate with lower compulsive usage, it suggests either (a) healthier people have less need for escapist scrolling, or (b) the time spent exercising/sleeping displaces screen time. Both are actionable.\n","metadata":{}},{"cell_type":"code","source":"# â”€â”€ Health Metrics vs DSI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nhealth_cols = ['exercise_hours_per_week', 'sleep_hours_per_night',\n               'daily_steps_count', 'body_mass_index']\navail_health = [c for c in health_cols if c in df.columns and 'doom_scroll_index' in df.columns]\n\nif avail_health:\n    n = len(avail_health)\n    fig, axes = plt.subplots(1, n, figsize=(5.5 * n, 5))\n    if n == 1:\n        axes = [axes]\n\n    health_colors = [P['mint'], P['blue'], P['gold'], P['coral']]\n\n    for i, hcol in enumerate(avail_health):\n        ax = axes[i]\n        s = df[[hcol, 'doom_scroll_index']].dropna().sample(min(25000, len(df)), random_state=42)\n\n        cmap_h = LinearSegmentedColormap.from_list('', ['#F8F9FA', health_colors[i]])\n        ax.hexbin(s[hcol], s['doom_scroll_index'], gridsize=30, mincnt=1,\n                  cmap=cmap_h, edgecolors='none')\n\n        # Binned means overlay\n        s['bin'] = pd.qcut(s[hcol], 20, labels=False, duplicates='drop')\n        bin_means = s.groupby('bin')[[hcol, 'doom_scroll_index']].mean()\n        ax.plot(bin_means[hcol], bin_means['doom_scroll_index'], 'o-',\n                color=P['text'], lw=2, markersize=4, alpha=0.9,\n                markeredgecolor='white', markeredgewidth=1)\n\n        r, p = stats.pearsonr(s[hcol], s['doom_scroll_index'])\n        direction = 'â†—' if r > 0 else 'â†˜'\n        ax.text(0.05, 0.95, f'{direction} r = {r:.3f}', transform=ax.transAxes,\n                fontsize=10, va='top', family='monospace', fontweight='bold',\n                bbox=dict(facecolor='white', alpha=0.85, edgecolor=P['grid'], boxstyle='round'))\n\n        ax.set_xlabel(hcol.replace('_', ' ').title(), fontsize=9)\n        ax.set_ylabel('Doom-Scroll Index' if i == 0 else '', fontsize=9)\n        ax.set_title(hcol.replace('_', ' ').title(), fontsize=11,\n                     color=health_colors[i], fontweight='bold')\n\n    fig.suptitle('Health & Lifestyle vs Doom-Scrolling',\n                 fontsize=15, fontweight='bold', y=1.04)\n    section_footer(fig)\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T19:53:51.612375Z","iopub.execute_input":"2026-02-02T19:53:51.612767Z","iopub.status.idle":"2026-02-02T19:53:53.245124Z","shell.execute_reply.started":"2026-02-02T19:53:51.612727Z","shell.execute_reply":"2026-02-02T19:53:53.24416Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**ğŸ“Œ Insight:** Negative r values between exercise/sleep/steps and DSI support the \"health buffer\" hypothesis. The binned-mean white lines show the trend shape â€” is it linear or does the benefit plateau? If exercise shows diminishing returns after a certain threshold, that's nuanced and interesting.\n","metadata":{}},{"cell_type":"markdown","source":"---\n# Question 8: What features most strongly predict someone's happiness?\n\n**What we're looking at:**\n- Mutual information scores: a model-free metric that captures both linear and nonlinear relationships\n- Every numeric feature ranked by how much information it provides about `self_reported_happiness`\n- Top features highlighted to reveal what matters most\n\n**Why it matters:** This is the culminating insight. After all the visual exploration, we now quantify: across all 50+ features, what actually moves the needle on happiness? Is it screen time? Sleep? Social activity? Income?\n","metadata":{}},{"cell_type":"code","source":"target = 'self_reported_happiness'\nif target in df.columns:\n    feat_cols = [c for c in num_cols + ['doom_scroll_index'] if c != target and c in df.columns]\n    feat_cols = list(set(feat_cols))\n\n    mi_sample = df[feat_cols + [target]].dropna().sample(min(50000, len(df)), random_state=42)\n    X_mi = mi_sample[feat_cols]\n    y_mi = mi_sample[target]\n\n    mi_scores = mutual_info_regression(X_mi, y_mi, random_state=42, n_neighbors=5)\n    mi_df = pd.DataFrame({'feature': feat_cols, 'MI': mi_scores}).sort_values('MI', ascending=True)\n\n    fig, ax = plt.subplots(figsize=(13, max(8, len(mi_df) * 0.32)))\n\n    # Color: top 5 = accent, rest = gray\n    n_feat = len(mi_df)\n    colors_mi = [P['subtle']] * n_feat\n    for i in range(1, 6):\n        if n_feat - i >= 0:\n            colors_mi[n_feat - i] = IG_GRAD[i - 1]\n\n    bars = ax.barh(range(n_feat), mi_df['MI'].values, color=colors_mi,\n                    edgecolor='white', height=0.7)\n    ax.set_yticks(range(n_feat))\n    ax.set_yticklabels(mi_df['feature'].values, fontsize=8)\n\n    # Annotate top 5\n    for i, (_, row) in enumerate(mi_df.tail(5).iterrows()):\n        pos = n_feat - 5 + i\n        ax.text(row['MI'] + mi_df['MI'].max() * 0.01, pos,\n                f\"  {row['MI']:.4f}\", va='center', fontsize=9,\n                color=P['text'], fontweight='bold')\n\n    ax.set_xlabel('Mutual Information Score', fontsize=11)\n    ax.set_title('What Predicts Happiness? â€” Mutual Information Ranking',\n                 fontsize=14, pad=15)\n    ax.grid(True, axis='x', alpha=0.3)\n    section_footer(fig)\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T19:53:53.246342Z","iopub.execute_input":"2026-02-02T19:53:53.2466Z","iopub.status.idle":"2026-02-02T19:54:11.056455Z","shell.execute_reply.started":"2026-02-02T19:53:53.246577Z","shell.execute_reply":"2026-02-02T19:54:11.054956Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**ğŸ“Œ Insight:** The highlighted top features are the strongest predictors of happiness in this dataset. If `perceived_stress_score` and `doom_scroll_index` rank high, it validates the entire investigation â€” the compulsive usage pattern we identified genuinely tracks with worse life outcomes. If lifestyle factors like `sleep_hours_per_night` or `exercise_hours_per_week` also rank high, it reinforces the health buffer finding from Question 7.\n","metadata":{}},{"cell_type":"markdown","source":"---\n# Appendix: Full Correlation Architecture\n\n**What we're looking at:**\n- Hierarchically clustered correlation matrix across all numeric features\n- Features reordered by dendrogram so correlated groups sit together\n- Strong correlations (|r| > 0.3) annotated\n\n**Why it's here:** This is the reference map for anyone who wants to dig deeper. It reveals feature families and unexpected associations at a glance.\n","metadata":{}},{"cell_type":"code","source":"# â”€â”€ Clustered Correlation Matrix â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nall_num = [c for c in num_cols + ['doom_scroll_index'] if c in df.columns]\ncorr_sample = df[all_num].dropna().sample(min(80000, len(df)), random_state=42)\ncorr = corr_sample.corr()\n\ndist = pdist(corr.values, metric='euclidean')\nlink = linkage(dist, method='ward')\norder = dendrogram(link, no_plot=True)['leaves']\ncorr_ordered = corr.iloc[order, order]\n\ncmap_corr = LinearSegmentedColormap.from_list(\n    'diverge', [P['cyan'], '#F8F9FA', P['coral']]\n)\n\nfig, ax = plt.subplots(figsize=(18, 15))\nmask = np.triu(np.ones_like(corr_ordered, dtype=bool), k=1)\nim = ax.imshow(corr_ordered.where(~mask).values, cmap=cmap_corr,\n               vmin=-1, vmax=1, aspect='auto')\n\nax.set_xticks(range(len(corr_ordered)))\nax.set_xticklabels(corr_ordered.columns, rotation=90, fontsize=7)\nax.set_yticks(range(len(corr_ordered)))\nax.set_yticklabels(corr_ordered.index, fontsize=7)\n\nfor i in range(len(corr_ordered)):\n    for j in range(i):\n        val = corr_ordered.iloc[i, j]\n        if abs(val) > 0.3:\n            ax.text(j, i, f'{val:.2f}', ha='center', va='center',\n                    fontsize=5.5, fontweight='bold',\n                    color=P['text'] if abs(val) > 0.5 else P['muted'])\n\ncbar = fig.colorbar(im, ax=ax, shrink=0.6, pad=0.02)\ncbar.set_label('Pearson r', fontsize=10)\nax.set_title('Hierarchically Clustered Correlation Matrix (|r| > 0.3 annotated)',\n             pad=20, fontsize=14)\nsection_footer(fig)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T19:54:11.057862Z","iopub.execute_input":"2026-02-02T19:54:11.058404Z","iopub.status.idle":"2026-02-02T19:54:14.367686Z","shell.execute_reply.started":"2026-02-02T19:54:11.058364Z","shell.execute_reply":"2026-02-02T19:54:14.366401Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**ğŸ“Œ Insight:** Blocks of warm color along the diagonal indicate feature families that move together (e.g., all Instagram time metrics cluster tightly). Off-diagonal warm spots reveal cross-domain links â€” like health metrics correlating with usage patterns. The `doom_scroll_index` row/column shows how our engineered feature relates to everything else.\n","metadata":{}},{"cell_type":"markdown","source":"---\n# ğŸ Investigation Summary\n\n### What we asked and what the data told us:\n\n| # | Question | Key Finding |\n|---|----------|-------------|\n| 1 | **Who are these users?** | Review the demographic panels â€” check for generational and geographic balance |\n| 2 | **Where does screen time go?** | Heavy users shift toward passive surfaces (Reels, Feed) while Messages stays flat |\n| 3 | **Can we detect doom-scrollers?** | The DSI composite index captures passive/compulsive behavior better than raw minutes |\n| 4 | **Does it predict worse wellbeing?** | DSI correlates with higher stress, lower happiness, and less sleep â€” and outperforms raw minutes |\n| 5 | **Is there a tipping point?** | The curves bend at a specific DSI value â€” usage below it is benign, above it shows real impact |\n| 6 | **What personas exist?** | 5 distinct archetypes from clustering, each with unique behavioral DNA |\n| 7 | **Does health help?** | Exercise and sleep show inverse relationships with compulsive usage |\n| 8 | **What predicts happiness?** | Stress, sleep, and the doom-scroll index rank among the top predictors |\n\n---\n*Notebook: Instagram Behavioral Genome Â· Built with matplotlib, seaborn, plotly, scikit-learn*\n","metadata":{}}]}